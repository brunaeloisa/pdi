<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.17">
<meta name="author" content="AndrÃ© Varela, Bruna Soares">
<title>Processamento Digital de Imagens (DCA0445)</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/styles/github.min.css">
</head>
<body class="article toc2 toc-left data-line-1">
<div id="header">
<h1>Processamento Digital de Imagens (DCA0445)</h1>
<div class="details">
<span id="author" class="author">AndrÃ© Varela</span><br>
<span id="email" class="email"><a href="mailto:andre.varela.104@ufrn.edu.br" data-href="mailto:andre.varela.104@ufrn.edu.br">andre.varela.104@ufrn.edu.br</a></span><br>
<span id="author2" class="author">Bruna Soares</span><br>
<span id="email2" class="email"><a href="mailto:brunaeloisa7@gmail.com" data-href="mailto:brunaeloisa7@gmail.com">brunaeloisa7@gmail.com</a></span><br>
</div>
<div id="toc" class="toc2">
<div id="toctitle">SumÃ¡rio</div>
<ul class="sectlevel1">
<li><a href="#_sobre_o_opencv">Sobre o OpenCV</a></li>
<li><a href="#_1Âª_unidade">1Âª Unidade</a>
<ul class="sectlevel2">
<li><a href="#_2_2_exercÃ­cios">2.2. ExercÃ­cios</a>
<ul class="sectlevel3">
<li><a href="#_2_2_1_regiÃ£o_negativa">2.2.1 RegiÃ£o negativa</a></li>
<li><a href="#_2_2_2_troca_de_quadrantes">2.2.2 Troca de quadrantes</a></li>
</ul>
</li>
<li><a href="#_3_2_exercÃ­cios">3.2. ExercÃ­cios</a>
<ul class="sectlevel3">
<li><a href="#_3_2_1_rotulaÃ§Ã£o_de_objetos">3.2.1 RotulaÃ§Ã£o de objetos</a></li>
<li><a href="#_3_2_2_contagem_de_objetos">3.2.2 Contagem de objetos</a></li>
</ul>
</li>
<li><a href="#_4_2_exercÃ­cios">4.2. ExercÃ­cios</a>
<ul class="sectlevel3">
<li><a href="#_4_2_1_equalizaÃ§Ã£o_de_histograma">4.2.1 EqualizaÃ§Ã£o de histograma</a></li>
<li><a href="#_4_2_2_detector_de_movimento">4.2.2 Detector de movimento</a></li>
</ul>
</li>
<li><a href="#_5_2_exercÃ­cios">5.2. ExercÃ­cios</a>
<ul class="sectlevel3">
<li><a href="#_5_2_1_laplaciano_do_gaussiano">5.2.1 Laplaciano do gaussiano</a></li>
</ul>
</li>
<li><a href="#_6_1_exercÃ­cios">6.1. ExercÃ­cios</a>
<ul class="sectlevel3">
<li><a href="#_6_1_1_tilt_shift_em_imagem">6.1.1 <em>Tilt-shift</em> em imagem</a></li>
<li><a href="#_6_1_2_tilt_shift_em_vÃ­deo">6.1.2 <em>Tilt-shift</em> em vÃ­deo</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_2Âª_unidade">2Âª Unidade</a>
<ul class="sectlevel2">
<li><a href="#_7_2_exercÃ­cios">7.2. ExercÃ­cios</a>
<ul class="sectlevel3">
<li><a href="#_7_2_1_filtro_homomÃ³rfico">7.2.1 Filtro homomÃ³rfico</a></li>
</ul>
</li>
<li><a href="#_8_3_exercÃ­cios">8.3 ExercÃ­cios</a>
<ul class="sectlevel3">
<li><a href="#_8_3_1_pontilhismo">8.3.1 Pontilhismo</a></li>
</ul>
</li>
<li><a href="#_9_2_exercÃ­cios">9.2 ExercÃ­cios</a>
<ul class="sectlevel3">
<li><a href="#_9_2_1_clusterizaÃ§Ã£o_com_k_means">9.2.1 ClusterizaÃ§Ã£o com <em>k-means</em></a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_3Âª_unidade">3Âª unidade</a>
<ul class="sectlevel2">
<li><a href="#_projeto_final_scanner_inteligente">Projeto final (<em>Scanner</em> inteligente)</a>
<ul class="sectlevel3">
<li><a href="#_proposiÃ§Ã£o">ProposiÃ§Ã£o</a></li>
<li><a href="#_implementaÃ§Ã£o">ImplementaÃ§Ã£o</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div class="sect1 data-line-10">
<h2 id="_sobre_o_opencv">Sobre o OpenCV</h2>
<div class="sectionbody">
<div class="paragraph data-line-12">
<p>Para a execuÃ§Ã£o dos cÃ³digos citados nesta pÃ¡gina foi utilizado o OpenCV (<em>Open Source Computer Vision</em>), que Ã© uma biblioteca <em>open-source</em> muito usada em aplicaÃ§Ãµes de visÃ£o computacional, processamento de imagens e vÃ­deos. Na linguagem Python, esta biblioteca possui uma boa integraÃ§Ã£o com o mÃ³dulo numÃ©rico NumPy, o qual oferece suporte para criaÃ§Ã£o de matrizes multi-dimensionais e um amplo leque de funÃ§Ãµes para manipulaÃ§Ã£o de <em>arrays</em>. Nas versÃµes mais recentes do OpenCV, esses objetos <code>ndarray</code> jÃ¡ sÃ£o utilizados nativamente como uma forma de otimizaÃ§Ã£o.</p>
</div>
</div>
</div>
<div class="sect1 data-line-14">
<h2 id="_1Âª_unidade">1Âª Unidade</h2>
<div class="sectionbody">
<div class="sect2 data-line-16">
<h3 id="_2_2_exercÃ­cios">2.2. ExercÃ­cios</h3>
<div class="sect3 data-line-18">
<h4 id="_2_2_1_regiÃ£o_negativa">2.2.1 RegiÃ£o negativa</h4>
<div class="ulist data-line-19">
<ul>
<li class="data-line-19">
<p>Implementar um programa capaz de solicitar ao usuÃ¡rio as coordenadas de dois pontos P1 e P2 localizados dentro dos limites do tamanho da imagem e exibir que lhe for fornecida. Entretanto, a regiÃ£o definida pelo retÃ¢ngulo de vÃ©rtices opostos definidos pelos pontos P1 e P2 serÃ¡ exibida com o negativo da imagem na regiÃ£o correspondente.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-21">
<p>A imagem utilizada como base estÃ¡ sendo mostrada na Figura 1.</p>
</div>
<div class="imageblock data-line-24">
<div class="content">
<img src="./imagens/biel.png" alt="biel">
</div>
<div class="title">Figure 1. biel.png</div>
</div>
<div class="paragraph data-line-26">
<p>Apesar da imagem em questÃ£o ser representada em tons de cinza, a soluÃ§Ã£o deste exercÃ­cio foi pensada para uma imagem genÃ©rica e, portanto, optamos por ler a imagem em matrizes de cores utilizando a flag <code>cv2.IMREAD_COLOR</code>.</p>
</div>
<div class="listingblock data-line-29">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">img_orig = cv2.imread('biel.png', cv2.IMREAD_COLOR)</code></pre>
</div>
</div>
<div class="paragraph data-line-33">
<p>ExtraÃ­mos, entÃ£o, as dimensÃµes da imagem e o nÃºmero de canais utilizando o atributo <code>shape</code>.</p>
</div>
<div class="listingblock data-line-36">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">altura, largura, canais = img_orig.shape</code></pre>
</div>
</div>
<div class="paragraph data-line-40">
<p>Na sequÃªncia, solicitamos ao usuÃ¡rio as coordenadas dos pontos de interesse P1 e P2. A partir das coordenadas fornecidas, percorremos os pixels que compÃµem o retÃ¢ngulo entre esses dois pontos, invertendo os tons de todos os canais.</p>
</div>
<div class="listingblock data-line-43">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">for i in range(min(p1x, p2x), max(p1x, p2x)+1):
    for j in range(min(p1y, p2y), max(p1y, p2y)+1):
        for k in range(canais):
            img_neg[i,j,k] = 255 - img_neg[i,j,k]</code></pre>
</div>
</div>
<div class="paragraph data-line-50">
<p>Para exibir o resultado, precisou-se utilizar a funÃ§Ã£o <code>cv2_imshow()</code>, a qual Ã© uma versÃ£o compatÃ­vel com o Google Colab de <code>cv2.imshow()</code>. O resultado final tomando-se a Figura 1 como entrada e considerando os pontos P1=(90, 190) e P2=(220, 120) estÃ¡ exposto na Figura 2.</p>
</div>
<div class="imageblock data-line-53">
<div class="content">
<img src="./imagens/resultados/biel_neg.png" alt="biel neg">
</div>
<div class="title">Figure 2. RegiÃ£o negativa em imagem sugerida</div>
</div>
<div class="paragraph data-line-55">
<p>Aplicando como entrada uma imagem colorida, obtivemos o seguinte resultado.</p>
</div>
<div class="imageblock data-line-58">
<div class="content">
<img src="./imagens/resultados/beatles_neg.png" alt="beatles neg">
</div>
<div class="title">Figure 3. RegiÃ£o negativa em imagem colorida</div>
</div>
</div>
<div class="sect3 data-line-62">
<h4 id="_2_2_2_troca_de_quadrantes">2.2.2 Troca de quadrantes</h4>
<div class="ulist data-line-63">
<ul>
<li class="data-line-63">
<p>Implementar um programa que troque os quadrantes em diagonal na imagem.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-65">
<p>Da mesma forma que o exercÃ­cio anterior, lemos a imagem e extraÃ­mos os seus dados essenciais. Vale ressaltar que para que a troca dos quadrantes seja possÃ­vel as dimensÃµes da imagem devem ser pares. Adicionamos, portanto, duas condicionais: caso uma das duas dimensÃµes da imagem seja Ã­mpar, descartaremos a Ãºltima linha e/ou coluna do <em>array</em>.</p>
</div>
<div class="listingblock data-line-68">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">if altura % 2 != 0:
    altura -= 1
    img_orig = img_orig[:-1,::]
if largura % 2 != 0:
    largura -= 1
    img_orig = img_orig[::,:-1]</code></pre>
</div>
</div>
<div class="paragraph data-line-77">
<p>Em seguida, calculamos a metade das dimensÃµes para uso posterior e criamos um novo <em>array</em> para armazenar a imagem trocada.</p>
</div>
<div class="paragraph data-line-79">
<p>Por fim, invertemos os quadrantes da figura selecionando as regiÃµes da imagem de origem e atribuindo-as Ã s suas novas posiÃ§Ãµes utilizando <em>slicing</em> de <em>arrays</em>, como mostra o trecho de cÃ³digo abaixo.</p>
</div>
<div class="listingblock data-line-82">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># quarto quadrante = segundo quadrante
img_trocada[meia_altura:,meia_largura:] = img_orig[:meia_altura,:meia_largura]
# terceiro quadrante = primeiro quadrante
img_trocada[meia_altura:,:meia_largura] = img_orig[:meia_altura,meia_largura:]
# primeiro quadrante = terceiro quadrante
img_trocada[:meia_altura,meia_largura:] = img_orig[meia_altura:,:meia_largura]
# segundo quadrante = quarto quadrante
img_trocada[:meia_altura,:meia_largura] = img_orig[meia_altura:,meia_largura:]</code></pre>
</div>
</div>
<div class="paragraph data-line-93">
<p>Exibimos, entÃ£o, o resultado armazenado em <code>img_trocada</code>, obtendo a figura final, como pode ser visto a seguir.</p>
</div>
<div class="imageblock data-line-96">
<div class="content">
<img src="./imagens/resultados/biel_trocada.png" alt="biel trocada">
</div>
<div class="title">Figure 4. Troca de quadrantes em imagem sugerida</div>
</div>
<div class="paragraph data-line-98">
<p>Como a lÃ³gica do programa foi pensada para uma imagem genÃ©rica, aplicou-se uma figura colorida com dimensÃµes de 814 x 543 pixels, a fim de verificar o seu funcionamento. O resultado Ã© apresentado na Figura 5.</p>
</div>
<div class="imageblock data-line-101">
<div class="content">
<img src="./imagens/resultados/lago_trocada.png" alt="lago trocada">
</div>
<div class="title">Figure 5. Troca de quadrantes em imagem colorida</div>
</div>
<div class="paragraph data-line-103">
<p>O cÃ³digo completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/trocaregioes.ipynb" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/trocaregioes.ipynb">trocaregioes.ipynb</a>.</p>
</div>
</div>
</div>
<div class="sect2 data-line-105">
<h3 id="_3_2_exercÃ­cios">3.2. ExercÃ­cios</h3>
<div class="sect3 data-line-107">
<h4 id="_3_2_1_rotulaÃ§Ã£o_de_objetos">3.2.1 RotulaÃ§Ã£o de objetos</h4>
<div class="ulist data-line-108">
<ul>
<li class="data-line-108">
<p>Observando-se o programa <a href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/labeling.cpp" data-href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/labeling.cpp">labeling.cpp</a> como exemplo, Ã© possÃ­vel verificar que caso existam mais de 255 objetos na cena, o processo de rotulaÃ§Ã£o poderÃ¡ ficar comprometido. Identifique a situaÃ§Ã£o em que isso ocorre e proponha uma soluÃ§Ã£o para este problema.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-110">
<p>Existe uma limitaÃ§Ã£o de valores em 255 tons de cinza distintos em razÃ£o do tipo de variÃ¡vel que estÃ¡ sendo utilizada (<code>unsigned char</code>). A proposta de soluÃ§Ã£o Ã© utilizar uma matriz do tipo <code>int</code> ou <code>float</code> para aumentar as possibilidades de contabilizaÃ§Ã£o. Para alÃ©m disso, poderÃ­amos tambÃ©m adaptar o algoritmo para o sistema RGB, possibilitando a criaÃ§Ã£o de 255Â³ rÃ³tulos, que sÃ£o diferenciÃ¡veis na visÃ£o computacional.</p>
</div>
</div>
<div class="sect3 data-line-112">
<h4 id="_3_2_2_contagem_de_objetos">3.2.2 Contagem de objetos</h4>
<div class="ulist data-line-113">
<ul>
<li class="data-line-113">
<p>Aprimore o algoritmo de contagem apresentado para identificar regiÃµes com ou sem buracos internos que existam na cena. Assuma que objetos com mais de um buraco podem existir. Inclua suporte no seu algoritmo para nÃ£o contar bolhas que tocam as bordas da imagem. NÃ£o se pode presumir, a priori, que elas tenham buracos ou nÃ£o.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-115">
<p>Como devemos levar em consideraÃ§Ã£o que podem existir bolhas com mais de um buraco, a imagem a ser utilizada como base para este cÃ³digo Ã© mostrada na Figura 6.</p>
</div>
<div class="imageblock data-line-118">
<div class="content">
<img src="./imagens/bolhas.png" alt="bolhas">
</div>
<div class="title">Figure 6. Entrada do programa</div>
</div>
<div class="paragraph data-line-120">
<p>Inicialmente, lÃª-se o arquivo e as dimensÃµes da imagem. Logo apÃ³s, percorremos todos os pixels de borda, aplicando um <em>flood fill</em> da cor do fundo (tom 0) sempre que nos depararmos com um pixel branco no percurso.</p>
</div>
<div class="listingblock data-line-123">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">for i in range(altura):
    if img_aux[i,0] == 255:
        cv2.floodFill(img_aux, None, (0,i), 0)
        num_bolhas_borda += 1
    if img_aux[i,largura-1] == 255:
        cv2.floodFill(img_aux, None, (largura-1,i), 0)
        num_bolhas_borda += 1
for j in range(largura):
    if img_aux[0,j] == 255:
        cv2.floodFill(img_aux, None, (j,0), 0)
        num_bolhas_borda += 1
    if img_aux[altura-1,j] == 255:
        cv2.floodFill(img_aux, None, (j,altura-1), 0)
        num_bolhas_borda += 1</code></pre>
</div>
</div>
<div class="paragraph data-line-140">
<p>A variÃ¡vel <code>num_bolhas_borda</code> armazena o nÃºmero de bolhas que foram excluÃ­das da figura e Ã© incrementada sempre que chamamos a funÃ§Ã£o <code>cv2.floodFill()</code>. A nova imagem, dada por <code>img_aux</code>, Ã© mostrada na Figura 7.</p>
</div>
<div class="imageblock data-line-143">
<div class="content">
<img src="./imagens/resultados/bolhas1.png" alt="bolhas1">
</div>
<div class="title">Figure 7. Imagem sem bolhas de borda</div>
</div>
<div class="paragraph data-line-145">
<p>Na sequÃªncia, faremos o <em>labeling</em> das bolhas presentes na nova imagem. Para isso, percorremos todos os pixels da figura, procurando novamente por pixels brancos. Ao encontrarmos uma nova bolha, incrementamos a variÃ¡vel <code>num_bolhas</code> e aplicamos o <em>flood fill</em> com o tom correspondente ao valor atual do contador.</p>
</div>
<div class="listingblock data-line-148">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">num_bolhas = 0
for i in range(altura):
    for j in range(largura):
        if img_aux[i,j] == 255:
            num_bolhas += 1
            cv2.floodFill(img_aux, None, (j,i), num_bolhas)</code></pre>
</div>
</div>
<div class="paragraph data-line-157">
<p>Ao fim deste processo, conheceremos a quantidade total de bolhas e cada uma delas estarÃ¡ sendo representada por um tom de cinza diferente, como exposto na Figura 8.</p>
</div>
<div class="imageblock data-line-160">
<div class="content">
<img src="./imagens/resultados/bolhas2.png" alt="bolhas2">
</div>
<div class="title">Figure 8. Labeling das bolhas</div>
</div>
<div class="paragraph data-line-162">
<p>Para realizar a contagem das bolhas com buracos, iniciamos pintando o fundo de branco, aplicando um <em>flood fill</em> no ponto (0,0), de modo que apenas os buracos apresentem tom de cinza 0.</p>
</div>
<div class="imageblock data-line-165">
<div class="content">
<img src="./imagens/resultados/bolhas3.png" alt="bolhas3">
</div>
<div class="title">Figure 9. AlteraÃ§Ã£o da cor de fundo</div>
</div>
<div class="paragraph data-line-167">
<p>Varremos novamente a figura, dessa vez buscando os pixels pretos e aplicando neles um <em>flood fill</em> com a cor do fundo. Verificamos, entÃ£o, o pixel antecedente: caso este seja diferente de 220, incrementamos a variÃ¡vel <code>num_bolhas_buracos</code> e pintamos a bolha que contÃ©m o buraco em questÃ£o com o tom de cinza 220; caso contrÃ¡rio, temos que a bolha jÃ¡ foi contabilizada.</p>
</div>
<div class="listingblock data-line-170">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">num_bolhas_buracos = 0
for i in range(1, altura):
    for j in range(1, largura):
        if img_aux[i,j] == 0:
            cv2.floodFill(img_aux, None, (j,i), 255)
            if img_aux[i,j-1] != 220:
                cv2.floodFill(img_aux, None, (j-1,i), 220)
                num_bolhas_buracos += 1</code></pre>
</div>
</div>
<div class="paragraph data-line-181">
<p>O trecho de cÃ³digo acima garantirÃ¡ que nenhuma bolha seja contabilizada mais de uma vez e farÃ¡ com que as bolhas com buracos fiquem destacadas na figura, como vemos abaixo.</p>
</div>
<div class="imageblock data-line-184">
<div class="content">
<img src="./imagens/resultados/bolhas4.png" alt="bolhas4">
</div>
<div class="title">Figure 10. IndentificaÃ§Ã£o das bolhas com buracos</div>
</div>
<div class="paragraph data-line-186">
<p>Obtemos, enfim, a seguinte saÃ­da: <code>A figura tem 7 bolhas com buracos e 14 bolhas sem buracos</code>. O cÃ³digo completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/labeling.ipynb" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/labeling.ipynb">labeling.ipynb</a>.</p>
</div>
</div>
</div>
<div class="sect2 data-line-188">
<h3 id="_4_2_exercÃ­cios">4.2. ExercÃ­cios</h3>
<div class="sect3 data-line-190">
<h4 id="_4_2_1_equalizaÃ§Ã£o_de_histograma">4.2.1 EqualizaÃ§Ã£o de histograma</h4>
<div class="ulist data-line-191">
<ul>
<li class="data-line-191">
<p>Implementar um programa que deverÃ¡, para cada imagem capturada, realizar a equalizaÃ§Ã£o do histograma antes de exibir a imagem. Teste sua implementaÃ§Ã£o apontando a cÃ¢mera para ambientes com iluminaÃ§Ãµes variadas e observando o efeito gerado. Assuma que as imagens processadas serÃ£o em tons de cinza.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-193">
<p>Para realizar a captura de imagens atravÃ©s de uma cÃ¢mera (<em>webcam</em>), utilizamos a funÃ§Ã£o <code>cv2.VideoCapture()</code>.</p>
</div>
<div class="listingblock data-line-196">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)</code></pre>
</div>
</div>
<div class="paragraph data-line-200">
<p>Em segunda instÃ¢ncia, definimos o tamanho da tela de exibiÃ§Ã£o como <em>(512 x 480)</em> e o nÃºmero de tonalidades como 256, podendo apresentar valores de 0 a 255 (o segundo valor de <code>hist_range</code> Ã© <em>exclusive</em>).</p>
</div>
<div class="listingblock data-line-203">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">hist_size = 256
hist_w, hist_h = 512, 480
hist_range = (0, 256)</code></pre>
</div>
</div>
<div class="paragraph data-line-209">
<p>O parÃ¢metro <code>bin_w</code> define a largura ocupada por cada tom de cinza no histograma.</p>
</div>
<div class="listingblock data-line-212">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">bin_w = int(round(hist_w/hist_size))</code></pre>
</div>
</div>
<div class="paragraph data-line-216">
<p>Na sequÃªncia, convertemos cada frame capturado para escala de cinza e realizamos a equalizaÃ§Ã£o com a funÃ§Ã£o <code>cv.equalizeHist()</code>. A partir de agora, os processos ocorrerÃ£o em <em>loop</em> para todos os frames coletados em tempo real enquanto nÃ£o existir uma condiÃ§Ã£o de interrupÃ§Ã£o do programa.</p>
</div>
<div class="listingblock data-line-219">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
frame_eq = cv2.equalizeHist(frame)</code></pre>
</div>
</div>
<div class="paragraph data-line-224">
<p>ApÃ³s a equalizaÃ§Ã£o dos tons de cinza, faremos o cÃ¡lculo do histograma por meio da funÃ§Ã£o <code>cv2.calcHist()</code> empregando os parÃ¢metros previamente definidos como argumentos. Depois disso, normalizaremos os histogramas gerados com a funÃ§Ã£o <code>cv2.normalize()</code>.</p>
</div>
<div class="listingblock data-line-227">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">hist = cv2.calcHist([frame], [0], None, [hist_size], hist_range)
cv2.normalize(hist, hist, alpha=0, beta=hist_h, norm_type=cv2.NORM_MINMAX)

hist_eq = cv2.calcHist([frame_eq], [0], None, [hist_size], hist_range)
cv2.normalize(hist_eq, hist_eq, alpha=0, beta=hist_h, norm_type=cv2.NORM_MINMAX)</code></pre>
</div>
</div>
<div class="paragraph data-line-235">
<p>Com isso, criamos um conjunto de barras verticais (representaÃ§Ã£o de um trem de pulsos), que partem do eixo horizontal e vÃ£o atÃ© o ponto do valor calculado, a fim de representar a quantidade de pixels de cada tom de cinza do frame.</p>
</div>
<div class="listingblock data-line-238">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">for i in range(hist_size):
    cv2.line(histImage, (bin_w*i, hist_h - int(hist[i])), (bin_w*i, hist_h), 255, thickness=2)
    cv2.line(histImage_eq, (bin_w*i, hist_h - int(hist_eq[i])), (bin_w*i, hist_h), 255, thickness=2)</code></pre>
</div>
</div>
<div class="paragraph data-line-244">
<p>Exibimos, entÃ£o, a captura gerada pela <em>webcam</em> e o seu histograma:</p>
</div>
<div class="imageblock data-line-247">
<div class="content">
<img src="./imagens/resultados/normal.png" alt="normal">
</div>
<div class="title">Figure 11. Captura original da webcam</div>
</div>
<div class="paragraph data-line-249">
<p>ApÃ³s a aplicaÃ§Ã£o da equalizaÃ§Ã£o com o nosso programa, geramos o seguinte resultado:</p>
</div>
<div class="imageblock data-line-252">
<div class="content">
<img src="./imagens/resultados/equalizado.png" alt="equalizado">
</div>
<div class="title">Figure 12. Captura equalizada</div>
</div>
<div class="paragraph data-line-254">
<p>Nota-se que, como a captura foi realizada em um ambiente com boa iluminaÃ§Ã£o, o histograma se encontra concentrado em uma regiÃ£o mais clara. ApÃ³s equalizaÃ§Ã£o, percebemos uma distribuiÃ§ao mais uniforme das intensidades dos pixels na imagem gerada e um melhor constraste.</p>
</div>
<div class="paragraph data-line-256">
<p>Para apreciar melhor os resultados da equalizaÃ§Ã£o, vamos introduzir uma imagem com pouco contraste, sendo a captura realizada em local com baixa iluminaÃ§Ã£o. Note que os pixels estÃ£o agrupados, predominantemente, no lado mais a esquerda do histograma.</p>
</div>
<div class="imageblock data-line-259">
<div class="content">
<img src="./imagens/resultados/normal_escuro.png" alt="normal escuro">
</div>
<div class="title">Figure 13. Captura original da webcam em ambiente de baixa iluminaÃ§Ã£o</div>
</div>
<div class="paragraph data-line-261">
<p>Abaixo visualizamos o resultado da equalizaÃ§Ã£o e do novo histograma obtido. Verifica-se uma clara redistribuiÃ§Ã£o dos pixels imagem por todo o histograma.</p>
</div>
<div class="paragraph data-line-263">
<p>Apesar de conseguirmos visualizar melhor alguns detalhes da imagem equalizada em comparaÃ§Ã£o com a imagem escura, o ruÃ­do estÃ¡ muito presente.</p>
</div>
<div class="imageblock data-line-266">
<div class="content">
<img src="./imagens/resultados/equalizado_escuro.png" alt="equalizado escuro">
</div>
<div class="title">Figure 14. Captura equalizada em ambiente de baixa iluminaÃ§Ã£o</div>
</div>
<div class="paragraph data-line-268">
<p>O cÃ³digo completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/equalize.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/equalize.py">equalize.py</a>.</p>
</div>
</div>
<div class="sect3 data-line-270">
<h4 id="_4_2_2_detector_de_movimento">4.2.2 Detector de movimento</h4>
<div class="ulist data-line-271">
<ul>
<li class="data-line-271">
<p>Implementar um programa que deverÃ¡ continuamente calcular o histograma da imagem (apenas uma componente de cor Ã© suficiente) e comparÃ¡-lo com o Ãºltimo histograma calculado. Ative um alarme quando a diferenÃ§a entre estes ultrapassar um limiar prÃ©-estabelecido, utilizando a funÃ§Ã£o de comparaÃ§Ã£o que julgar conveniente.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-273">
<p>Este programa utiliza as alteraÃ§Ãµes no histograma dos frames do vÃ­deo capturado para detecÃ§Ã£o de movimentos. Partindo de uma estrutura semelhante ao do algoritmo anterior, calcularemos, para cada frame, o histograma da componente vermelha e o compararemos com o histograma que o antecedeu.</p>
</div>
<div class="paragraph data-line-275">
<p>Precisamos, inicialmente, separar os canais da imagem:</p>
</div>
<div class="listingblock data-line-278">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">bgr = cv2.split(frame)</code></pre>
</div>
</div>
<div class="paragraph data-line-282">
<p>Assim, calculamos e normalizamos o histograma atual do canal vermelho.</p>
</div>
<div class="listingblock data-line-285">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">hist_atual = cv2.calcHist(bgr, [2], None, [hist_size], hist_range)
cv2.normalize(hist_atual, hist_atual, alpha=0, beta=hist_h, norm_type=cv2.NORM_MINMAX)</code></pre>
</div>
</div>
<div class="paragraph data-line-290">
<p>Para a condiÃ§Ã£o inicial em que nÃ£o temos um frame anterior para comparaÃ§Ã£o, inicializamos uma variÃ¡vel <code>inicio = True</code> e atribuÃ­mos a <code>hist_anterior</code> o histograma atual que estÃ¡ sendo gerado. Terminada esta aÃ§Ã£o, a variÃ¡vel <code>inicio</code> passa a ser <code>False</code>.</p>
</div>
<div class="listingblock data-line-293">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">if inicio:
    hist_anterior = hist_atual.copy()
    inicio = False</code></pre>
</div>
</div>
<div class="paragraph data-line-299">
<p>Os histogramas sÃ£o comparados por meio da funÃ§Ã£o <code>compareHist()</code> e o resultado Ã© inserido na varivÃ¡vel <code>dif</code>, que representa o quanto o histograma do frame atual difere do anterior. Para tal, utilizamos a mÃ©trica <code>cv2.HISTCMP_BHATTACHARYYA</code>, a qual retorna valores entre 0.0 e 1.0, em que quanto mais prÃ³ximo de 0.0, maior Ã© a semelhanÃ§a entre as imagens.</p>
</div>
<div class="paragraph data-line-301">
<p>A partir disso, comparamos o valor encontrado (em %) ao limiar, que foi definido como 3 apÃ³s alguns testes para alcanÃ§ar a sensibilidade desejada. Quando este Ã© ultrapassado, uma mensagem de <code>Movimento detectado !</code> Ã© exibida na imagem.</p>
</div>
<div class="listingblock data-line-304">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">dif = cv2.compareHist(hist_anterior, hist_atual, cv2.HISTCMP_BHATTACHARYYA)
    if (100 * dif &gt; 3):
        cv2.putText(frame, "Movimento detectado!", (10, 30), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 255))</code></pre>
</div>
</div>
<div class="paragraph data-line-310">
<p>O resultado pode ser observado abaixo:</p>
</div>
<div class="imageblock data-line-313">
<div class="content">
<img src="./imagens/resultados/motiondetector.gif" alt="motiondetector">
</div>
<div class="title">Figure 15. DetecÃ§Ã£o de movimento</div>
</div>
<div class="paragraph data-line-315">
<p>O cÃ³digo completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/motiondetector.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/motiondetector.py">motiondetector.py</a>.</p>
</div>
</div>
</div>
<div class="sect2 data-line-317">
<h3 id="_5_2_exercÃ­cios">5.2. ExercÃ­cios</h3>
<div class="sect3 data-line-319">
<h4 id="_5_2_1_laplaciano_do_gaussiano">5.2.1 Laplaciano do gaussiano</h4>
<div class="ulist data-line-320">
<ul>
<li class="data-line-320">
<p>Utilizando o programa <a href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/filtroespacial.cpp" data-href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/filtroespacial.cpp">filtroespacial.cpp</a> como referÃªncia, implementar um programa que acrescente mais uma funcionalidade ao exemplo fornecido, permitindo que seja calculado o laplaciano do gaussiano das imagens capturadas. Compare o resultado desse filtro com a simples aplicaÃ§Ã£o do filtro laplaciano.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-322">
<p>Inicialmente, adaptamos o cÃ³digo citado no exercÃ­cio para a linguagem Python. Nele, o usuÃ¡rio pode escolher o efeito (mÃ³dulo, mÃ©dia, gaussiano, bordas horizontais, bordas verticais, laplaciano e <em>boost</em>) que deseja aplicar Ã s imagens capturadas em tempo real por meio das teclas do teclado.</p>
</div>
<div class="listingblock data-line-325">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">key = cv2.waitKey(10)

if key == 27:
    break
elif key == ord('a'):
    absolut = not absolut
  elif key == ord('m'):
    laplacian_gauss = False
    mask = np.float32(media)
  elif key == ord('g'):
    laplacian_gauss = False
    mask = np.float32(gauss)
  elif key == ord('h'):
    laplacian_gauss = False
    mask = np.float32(horizontal)
  elif key == ord('v'):
    laplacian_gauss = False
    mask = np.float32(vertical)
  elif key == ord('l'):
    laplacian_gauss = False
    mask = np.float32(laplacian)
  elif key == ord('b'):
    laplacian_gauss = False
    mask = np.float32(boost)
  elif key == ord('f'):
    laplacian_gauss = True
    mask = np.float32(gauss)
  elif key == ord('s'):
    cv2.imwrite('filtered_img.png', filtro_espacial)
    print("Imagem salva em filtered_img.png")</code></pre>
</div>
</div>
<div class="paragraph data-line-358">
<p>Nota-se que foram adicionadas duas novas funcionalidades ao programa: o filtro laplaciano do gaussiano e o salvamento em arquivo do frame atual com filtro aplicado.</p>
</div>
<div class="paragraph data-line-360">
<p>Para realizar o laplaciano do gaussiano, foi criada uma <em>flag</em> (<code>laplacian_gauss</code>) que retorna <code>True</code> quando o comando (tecla <em>f</em>) Ã© ativado. Dessa forma, primeiramente selecionamos a mÃ¡scara adequada para aplicar o filtro gaussiano e, em seguida, criamos uma expressÃ£o condicional baseada na <em>flag</em> em questÃ£o a fim de aplicar o laplaciano em cima da imagem jÃ¡ filtrada previamente.</p>
</div>
<div class="listingblock data-line-363">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
frame_gray = cv2.flip(frame_gray, 1)

frame_32f = np.float32(frame_gray)
filtered_frame = cv2.filter2D(frame_32f, -1, mask)

if laplacian_gauss:
    filtered_frame = cv2.filter2D(filtered_frame, -1, laplacian)</code></pre>
</div>
</div>
<div class="paragraph data-line-374">
<p>Quando outras opÃ§Ãµes de filtro sÃ£o escolhidas, a variÃ¡vel <code>laplacian_gauss</code> Ã© setada para <code>False</code> e, portanto, nenhum processo adicional Ã© realizado no frame.</p>
</div>
<div class="paragraph data-line-376">
<p>Na sequÃªncia, verificamos se os valores mostrados devem ser absolutos ou nÃ£o e transformamos os dados da matriz resultante que representa a imagem final em valores <code>unsigned</code> de 8 bits (0 a 255), para, sÃ³ entÃ£o, exibÃ­-la em uma janela, assim como a imagem original.</p>
</div>
<div class="listingblock data-line-379">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">if absolut:
    filtered_frame = np.abs(filtered_frame)

filtro_espacial = np.uint8(filtered_frame)

cv2.imshow('imagem original', frame_gray)
cv2.imshow('imagem com efeito', filtro_espacial)</code></pre>
</div>
</div>
<div class="paragraph data-line-389">
<p>Para comparar os efeitos do filtro laplaciano e laplaciano do gaussiano, utilizamos como entrada do programa a imagem exposta a seguir.</p>
</div>
<div class="imageblock data-line-392">
<div class="content">
<img src="./imagens/livro.png" alt="livro">
</div>
<div class="title">Figure 16. Imagem utilizada no programa</div>
</div>
<div class="paragraph data-line-394">
<p>O comparativo entre os dois resultados Ã© mostrado na Figura 17. Percebe-se que, ao aplicarmos o filtro gaussiano, que realiza um borramento, antes do laplaciano, atenuamos os ruÃ­dos da imagem tornando as bordas mais precisas na figura resultante, dando a impressÃ£o de uma imagem mais escura.</p>
</div>
<div class="imageblock data-line-397">
<div class="content">
<img src="./imagens/resultados/laplgauss_livro.png" alt="laplgauss livro">
</div>
<div class="title">Figure 17. Comparativo entre os filtros laplaciano e laplaciano do gaussiano</div>
</div>
<div class="paragraph data-line-399">
<p>O cÃ³digo completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/laplgauss.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/laplgauss.py">laplgauss.py</a>.</p>
</div>
</div>
</div>
<div class="sect2 data-line-401">
<h3 id="_6_1_exercÃ­cios">6.1. ExercÃ­cios</h3>
<div class="sect3 data-line-403">
<h4 id="_6_1_1_tilt_shift_em_imagem">6.1.1 <em>Tilt-shift</em> em imagem</h4>
<div class="ulist data-line-404">
<ul>
<li class="data-line-404">
<p>Utilizando o programa <a href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/addweighted.cpp" data-href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/addweighted.cpp">addweighted.cpp</a> como referÃªncia, implementar um programa no qual trÃªs ajustes deverÃ£o ser providos na tela da interface:</p>
<div class="ulist data-line-405">
<ul>
<li class="data-line-405">
<p>um ajuste para regular a altura da regiÃ£o central que entrarÃ¡ em foco;</p>
</li>
<li class="data-line-406">
<p>um ajuste para regular a forÃ§a de decaimento da regiÃ£o borrada;</p>
</li>
<li class="data-line-407">
<p>um ajuste para regular a posiÃ§Ã£o vertical do centro da regiÃ£o que entrarÃ¡ em foco. Finalizado o programa, a imagem produzida deverÃ¡ ser salva em arquivo.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph data-line-409">
<p>Para este cÃ³digo, inicialmente lemos a imagem de entrada do programa, armazenando-a na variÃ¡vel <code>img1</code>. Em seguida, a <code>img2</code> Ã© obtida aplicando-se cinco vezes o filtro da mÃ©dia com mÃ¡scara 5x5 na imagem original, a fim de alcanÃ§ar um efeito de borramento com intensidade desejÃ¡vel.</p>
</div>
<div class="listingblock data-line-412">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">img_32f = np.float32(img2)
for n in range(5):
    img_32f = cv2.filter2D(img_32f, -1, mask)
img2 = np.uint8(img_32f)</code></pre>
</div>
</div>
<div class="paragraph data-line-419">
<p>Criamos a janela e tambÃ©m as barras de ajuste solicitadas no exercÃ­cio utilizando a funÃ§Ã£o <code>cv2.createTrackbar()</code>, definindo um <em>slider</em> que inicia em 0 e pode ir atÃ© 100. Um dos parÃ¢metros exigidos para esta criaÃ§Ã£o Ã© uma funÃ§Ã£o de <em>callback</em>, a qual serÃ¡ chamada sempre que o usuÃ¡rio interagir com a barra de rolagem.</p>
</div>
<div class="listingblock data-line-422">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">cv2.namedWindow('Tiltshift')
cv2.createTrackbar('altura', 'Tiltshift', slider_inicial, slider_max, on_trackbar)
cv2.createTrackbar('centro', 'Tiltshift', slider_inicial, slider_max, on_trackbar)
cv2.createTrackbar('decaimento', 'Tiltshift', slider_inicial, slider_max, on_trackbar)</code></pre>
</div>
</div>
<div class="paragraph data-line-429">
<p>Ao chamar o <em>callback</em>, a funÃ§Ã£o apresentada acima automaticamente envia como argumento o valor atual da barra que foi ajustada. Por esse motivo, a funÃ§Ã£o <code>on_trackbar()</code> possui um argumento (<code>val</code>), mas este nÃ£o Ã© utilizado em seu interior, uma vez que optamos por usar a mesma funÃ§Ã£o de <em>callback</em> para todas as trÃªs barras. Como alternativa, coletamos a posiÃ§Ã£o atual de todos os <em>sliders</em> localmente, utilizando a funÃ§Ã£o <code>cv2.getTrackbarPos()</code> sempre que hÃ¡ alteraÃ§Ã£o.</p>
</div>
<div class="listingblock data-line-432">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def on_trackbar(val):
    global img_final

    slider_altura = cv2.getTrackbarPos('altura', 'Tiltshift')  # altura da regiÃ£o central
    slider_centro = cv2.getTrackbarPos('centro', 'Tiltshift')  # posiÃ§Ã£o vertical do centro
    slider_decaimento = cv2.getTrackbarPos('decaimento', 'Tiltshift')

    l1 = slider_centro - int(slider_altura/2)
    l2 = slider_centro + int(slider_altura/2)

    if l1 &gt;= 0 and l2 &lt;= 100:
        l1 = l1*height/100
        l2 = l2*height/100
    else:
        return

    aux = img1.copy()

    x = np.arange(height, dtype=np.float32)

    alpha = 0.5 * (np.tanh((x - l1)/(slider_decaimento+0.001)) - np.tanh((x - l2)/(slider_decaimento+0.001)))

    for i, element in enumerate(alpha):
        aux[i] = cv2.addWeighted(img1[i], element, img2[i], 1 - element, 0.0)

    cv2.imshow('Tiltshift', aux)
    img_final = aux</code></pre>
</div>
</div>
<div class="paragraph data-line-462">
<p>Para simular o efeito do <em>tilt-shift</em>, fazemos uma soma ponderada entre a imagem e sua versÃ£o borrada por meio da funÃ§Ã£o <code>cv2.addWeighted()</code>. Esse processo pode ser modelado por uma funÃ§Ã£o que define a regiÃ£o de desfoque ao longo do eixo vertical da imagem:</p>
</div>
<div class="stemblock data-line-465">
<div class="content">
\[\alpha(x) = \frac{1}{2}\left(\tanh{\frac{x-l_1}{d}}-\tanh{\frac{x-l_2}{d}} \right)\]
</div>
</div>
<div class="paragraph data-line-469">
<p>em que \(x\) representa as linhas da imagem, \(l_1\) e \(l_2\) sÃ£o os limites verticais para a regiÃ£o sem borramento cujo \(\alpha\) assume valor em torno de 0.5 e \(d\) indica a forÃ§a do decaimento da regiÃ£o normal para a regiÃ£o borrada.</p>
</div>
<div class="paragraph data-line-471">
<p>Para evitar o erro de divisÃ£o por zero, foi acrescido ao decaimento um valor de 0.001. AlÃ©m disso, antes de atribuir os novos parÃ¢metros, verificamos se os valores de altura da regiÃ£o sem borramento e o centro determinado sÃ£o compatÃ­veis para que o tamanho da imagem nÃ£o seja excedido.</p>
</div>
<div class="paragraph data-line-473">
<p>Ao final, exibimos na janela o resultado dessa combinaÃ§Ã£o linear e o armazenamos na variÃ¡vel global <code>img_final</code>, a fim de disponibilizar a imagem atual para ser salva em arquivo pelo comando <em>s</em>.</p>
</div>
<div class="paragraph data-line-475">
<p>Para verificar o funcionamento do cÃ³digo, utilizamos a imagem abaixo como entrada do programa.</p>
</div>
<div class="imageblock data-line-478">
<div class="content">
<img src="./imagens/cidade.jpg" alt="cidade">
</div>
<div class="title">Figure 18. Imagem de entrada para o <em>tilt-shift</em></div>
</div>
<div class="paragraph data-line-480">
<p>Ao realizarmos o ajuste das barras (altura: 70; centro: 48; decaimento: 22), chegamos no resultado apresentado na Figura 19.</p>
</div>
<div class="imageblock data-line-483">
<div class="content">
<img src="./imagens/resultados/img_tiltshift.png" alt="img tiltshift">
</div>
<div class="title">Figure 19. Imagem de saÃ­da para o <em>tilt-shift</em></div>
</div>
<div class="paragraph data-line-485">
<p>O cÃ³digo completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/tiltshift.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/tiltshift.py">tiltshift.py</a>.</p>
</div>
</div>
<div class="sect3 data-line-487">
<h4 id="_6_1_2_tilt_shift_em_vÃ­deo">6.1.2 <em>Tilt-shift</em> em vÃ­deo</h4>
<div class="ulist data-line-488">
<ul>
<li class="data-line-488">
<p>Implementar um programa capaz de processar um arquivo de vÃ­deo, produzir o efeito de <em>tilt-shift</em> nos quadros presentes e escrever o resultado em outro arquivo de vÃ­deo. A ideia Ã© criar um efeito de miniaturizaÃ§Ã£o de cenas. Descarte quadros em uma taxa que julgar conveniente para evidenciar o efeito de <em>stop motion</em>, comum em vÃ­deos desse tipo.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-490">
<p>Este caso Ã© uma aplicaÃ§Ã£o do exemplo acima, em que cada quadro do vÃ­deo Ã© processado como realizado anteriormente e alguns quadros sÃ£o descartados para criar um efeito de <em>stop motion</em>.</p>
</div>
<div class="paragraph data-line-492">
<p>O vÃ­deo utilizado para exemplificaÃ§Ã£o estÃ¡ disposto abaixo.</p>
</div>
<div class="videoblock data-line-495">
<div class="title">VÃ­deo original</div>
<div class="content">
<video src="./videos/video.mp4" controls>
Your browser does not support the video tag.
</video>
</div>
</div>
<div class="paragraph data-line-497">
<p>O algoritmo usarÃ¡ os parÃ¢metros de ajuste que sÃ£o fornecidos pelo usuÃ¡rio atravÃ©s da mudanÃ§a das barras de rolagem para calcular os valores de \(\alpha\), que Ã© um vetor do tamanho da imagem. Esse efeito, que a princÃ­pio Ã© gerado no frame inicial, repercutirÃ¡ por todos os frames restantes compondo o vÃ­deo final.</p>
</div>
<div class="listingblock data-line-500">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">cap = cv2.VideoCapture(arq_video)

# lÃª primeiro frame para ajustar parÃ¢metros do tilt-shift
ret, frame1 = cap.read()</code></pre>
</div>
</div>
<div class="paragraph data-line-507">
<p>Como o mesmo efeito serÃ¡ aplicado para todos os frames do vÃ­deo, uma vez definida a configuraÃ§Ã£o, nÃ£o serÃ¡ necessÃ¡rio recalcular os parÃ¢metros. Por esse motivo, tratamos o vetor \(\alpha\) como uma variÃ¡vel global que serÃ¡ constantemente atualizada pela funÃ§Ã£o de <em>callback</em> das barras para, enfim, ser utilizada dentro da funÃ§Ã£o <code>tiltshift()</code> repetidas vezes durante a criaÃ§Ã£o do vÃ­deo.</p>
</div>
<div class="paragraph data-line-509">
<p>Essa funÃ§Ã£o recebe como argumento o frame original e o frame borrado e retorna a imagem com o efeito do <em>tilt-shift</em> aplicado. O seu algoritmo segue a mesma lÃ³gica que o exercÃ­cio anterior.</p>
</div>
<div class="listingblock data-line-512">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def tiltshift(frame1, frame2):
    img = frame1.copy()

    for i, element in enumerate(alpha):
        img[i] = cv2.addWeighted(
            frame1[i], element, frame2[i], 1 - element, 0.0)

    return img</code></pre>
</div>
</div>
<div class="paragraph data-line-523">
<p>Como requisito do programa, a funÃ§Ã£o <code>salvarvideo()</code> grava o resultado do efeito aplicado no vÃ­deo. O objetivo da funÃ§Ã£o <code>cv2.VideoWriter()</code> Ã©, portanto, salvar o vÃ­deo final em arquivo, definindo o nome (<code>video_tiltshift.mp4</code>), extensÃ£o, taxa de frames por segundo e as dimensÃµes do vÃ­deo (iguais ao original).</p>
</div>
<div class="paragraph data-line-525">
<p>Para criar o efeito de <em>stop motion</em>, uma condiÃ§Ã£o <code>descarta</code> Ã© imposta e a variÃ¡vel <code>taxa</code> determinarÃ¡ esse descarte. Nesse processo, os frames que sÃ£o mÃºltiplos do valor armazenado em <code>taxa</code> devem ser mantidos, ou seja, a cada 8 frames do vÃ­deo original, extraÃ­mos 1 frame para o vÃ­deo final.</p>
</div>
<div class="listingblock data-line-528">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def salvarvideo(video):
    taxa = 8
    descarta = 0

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter('video_tiltshift.mp4', fourcc, 15, (width, height))

    cap = cv2.VideoCapture(video)

    while True:
        ret, frame1 = cap.read()
        if ret:
            if descarta == 0:
                frame2 = frame1.copy()

                # aplica borramento
                frame_32f = np.float32(frame2)
                for _ in range(5):
                    frame_32f = cv2.filter2D(frame_32f, -1, mask)
                frame2 = np.uint8(frame_32f)

                novo_frame = tiltshift(frame1, frame2)
                out.write(novo_frame)

                descarta += 1
                descarta = descarta % taxa
            else:
                descarta += 1
                descarta = descarta % taxa
        else:
            break

    out.release()
    print("VÃ­deo salvo como video_tiltshift.mp4")</code></pre>
</div>
</div>
<div class="paragraph data-line-565">
<p>Quando o usuÃ¡rio concluir a sua configuraÃ§Ã£o, ele pode pressionar a tecla <em>s</em> para salvar o resultado no arquivo de vÃ­deo <code>video_tiltshift.mp4</code>.</p>
</div>
<div class="listingblock data-line-568">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">k = cv2.waitKey(0)
if k == 27:
    cv2.destroyAllWindows()
elif k == ord('s'):
    salvarvideo(arq_video)
    cv2.destroyAllWindows()</code></pre>
</div>
</div>
<div class="paragraph data-line-577">
<p>O resultado do efeito <em>tilt-shift</em> no vÃ­deo mostrado previamente pode ser visualizado abaixo.</p>
</div>
<div class="imageblock data-line-580">
<div class="content">
<img src="./imagens/resultados/video_tiltshift.gif" alt="video tiltshift">
</div>
<div class="title">Figure 20. VÃ­deo resultante com <em>tilt-shift</em></div>
</div>
<div class="paragraph data-line-582">
<p>O cÃ³digo completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/tiltshiftvideo.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/tiltshiftvideo.py">tiltshiftvideo.py</a>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1 data-line-584">
<h2 id="_2Âª_unidade">2Âª Unidade</h2>
<div class="sectionbody">
<div class="sect2 data-line-586">
<h3 id="_7_2_exercÃ­cios">7.2. ExercÃ­cios</h3>
<div class="sect3 data-line-588">
<h4 id="_7_2_1_filtro_homomÃ³rfico">7.2.1 Filtro homomÃ³rfico</h4>
<div class="ulist data-line-589">
<ul>
<li class="data-line-589">
<p>Utilizando o programa <a href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/dft.cpp" data-href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/dft.cpp">dft.cpp</a> como referÃªncia, implementar o filtro homomÃ³rfico para melhorar imagens com iluminaÃ§Ã£o irregular. Assumindo que a imagem fornecida Ã© em tons de cinza, crie uma cena mal iluminada e ajuste os parÃ¢metros do filtro homomÃ³rfico para corrigir a iluminaÃ§Ã£o da melhor forma possÃ­vel.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-591">
<p>Neste exercÃ­cio, resolveremos problemas de distribuiÃ§Ã£o irregular de iluminaÃ§Ã£o em imagens utilizando o filtro homomÃ³rfico.</p>
</div>
<div class="paragraph data-line-593">
<p>O filtro homomÃ³rfico Ã© uma versÃ£o modificada do filtro gaussiano e atua sobre as componentes de reflectÃ¢ncia e iluminÃ¢ncia da imagem, a partir da seguinte equaÃ§Ã£o matemÃ¡tica:</p>
</div>
<div class="stemblock data-line-596">
<div class="content">
\[H(u,v)=(\gamma _{H}-\gamma _{L})(1-e^{-c(D^{2}(u,v)/D_{0}^{2})})+\gamma _{L}\]
</div>
</div>
<div class="paragraph data-line-600">
<p>Os parÃ¢metros do filtro (\(\gamma_{H}\), \(\gamma_{L}\), \(D_{0}\) e \(c\)) sÃ£o ajustados pelo usuÃ¡rio a fim atingir o resultado desejado: reduzir os efeitos causados pela mÃ¡ iluminaÃ§Ã£o na imagem original. O algoritmo implementado segue os passos descritos a seguir.</p>
</div>
<div class="paragraph data-line-602">
<p>Primeiramente, realiza-se o <em>padding</em> da imagem e aplica-se a transformada de Fourier, transferindo a anÃ¡lise para o domÃ­nio da frequÃªncia.</p>
</div>
<div class="paragraph data-line-604">
<p>Em seguida, desloca-se o centro do seu espectro, invertendo os quadrantes da imagem (AâD, BâC) por meio da funÃ§Ã£o <code>deslocaDFT()</code>.</p>
</div>
<div class="listingblock data-line-607">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def deslocaDFT(image):
    # se a imagem tiver tamanho Ã­mpar, recorta a regiao para evitar cÃ³pias de tamanho desigual
    image = image[0:(image.shape[0] &amp; -2), 0:(image.shape[1] &amp; -2)]

    cy = int(image.shape[0] / 2)
    cx = int(image.shape[1] / 2)

    # reorganiza os quadrantes da transformada
    # A B   -&gt;  D C
    # C D       B A
    A = image[:cy, :cx].copy()
    B = image[:cy, cx:].copy()
    C = image[cy:, :cx].copy()
    D = image[cy:, cx:].copy()

    # A &lt;-&gt; D
    image[:cy, :cx] = D
    image[cy:, cx:] = A

    # C &lt;-&gt; B
    image[:cy, cx:] = C
    image[cy:, :cx] = B

    return image</code></pre>
</div>
</div>
<div class="paragraph data-line-634">
<p>Na sequÃªncia, multiplica-se a funÃ§Ã£o na frequÃªncia pelo filtro (\(G(u,v) = H(u,v)F(u,v)\)) e retoma-se a configuraÃ§Ã£o original dos quadrantes da imagem invertendo nocamente os quadrantes.</p>
</div>
<div class="paragraph data-line-636">
<p>Finalmente, calcula-se a transformada inversa do produto e extrai-se a parte real da imagem no domÃ­nio espacial.</p>
</div>
<div class="paragraph data-line-638">
<p>Detendo-se ao filtro em si, definimos a funÃ§Ã£o <code>filtro_homomorfico()</code> que implementarÃ¡ o equacionamento matemÃ¡tico. Como comentado anteriormente, o algoritmo usarÃ¡ os parÃ¢metros do filtro, os quais sÃ£o fornecidos pelo usuÃ¡rio atravÃ©s do controle das barras deslizantes, que variam de 0 a 100.</p>
</div>
<div class="listingblock data-line-641">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def filtro_homomorfico():
    gammaH = cv2.getTrackbarPos('gama H', 'filtro homomorfico') / 100
    gammaL = cv2.getTrackbarPos('gama L', 'filtro homomorfico') / 100
    D0 = cv2.getTrackbarPos('D0', 'filtro homomorfico')
    c = cv2.getTrackbarPos('c', 'filtro homomorfico') / 10</code></pre>
</div>
</div>
<div class="paragraph data-line-649">
<p>A variÃ¡vel <code>tmp</code> corresponde Ã  matriz temporÃ¡ria utilizada para criar as componentes real e imaginÃ¡ria do filtro. Com isso, define-se <em>D(u,v)</em> e implemeta-se o filtro, exibindo o resultado em janela. Enfim, cria-se a matriz com as componentes do filtro e combina ambas em uma matriz multicanal complexa.</p>
</div>
<div class="listingblock data-line-652">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    tmp = np.zeros((dft_M, dft_N), dtype=np.float32)
    v = np.array(range(dft_N))

    for u in range(dft_M):
        D = np.sqrt((u-dft_M/2)**2 + (v-dft_N/2)**2)
        tmp[u] = (gammaH-gammaL)*(1-np.exp(-c*(D**2)/(0.0001+D0**2))) + gammaL

    cv2.imshow('filtro', tmp)

    comps = [tmp, tmp]
    filtro_h = cv2.merge(comps)

    return filtro_h</code></pre>
</div>
</div>
<div class="paragraph data-line-668">
<p>Para exemplificar, foi gerado um filtro com os seguintes parÃ¢metros: \(\gamma_{H}=89\), \(\gamma_{L}=35\), \(D_{0}=70\) e \(c=96\), obtendo o resultando abaixo. Verifica-se que este facilita a passagem de componentes de frequÃªncias mais altas (reflectÃ¢ncia) em detrimento das frequÃªncias mais baixas (iluminÃ¢ncia).</p>
</div>
<div class="imageblock data-line-671">
<div class="content">
<img src="./imagens/filtro_h.png" alt="filtro h">
</div>
<div class="title">Figure 21. Filtro homomÃ³rfico gerado pelos parÃ¢metros de entrada</div>
</div>
<div class="paragraph data-line-673">
<p>A imagem original utilizada de teste Ã© apresentada na Figura 22. Em seguida, Ã© mostrada a imagem apÃ³s a realizaÃ§Ã£o da filtragem homomÃ³rfica.</p>
</div>
<div class="imageblock data-line-676">
<div class="content">
<img src="./imagens/entrada_homomorfico.png" alt="entrada homomorfico">
</div>
<div class="title">Figure 22. Imagem original com problema de iluminaÃ§Ã£o</div>
</div>
<div class="imageblock data-line-679">
<div class="content">
<img src="./imagens/resultados/foto_corrigida.png" alt="foto corrigida">
</div>
<div class="title">Figure 23. Imagem com parÃ¢metros do filtro homomÃ³rfico aplicados</div>
</div>
<div class="paragraph data-line-681">
<p>O cÃ³digo completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/homomorfico.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/homomorfico.py">homomorfico.py</a>.</p>
</div>
</div>
</div>
<div class="sect2 data-line-683">
<h3 id="_8_3_exercÃ­cios">8.3 ExercÃ­cios</h3>
<div class="sect3 data-line-685">
<h4 id="_8_3_1_pontilhismo">8.3.1 Pontilhismo</h4>
<div class="ulist data-line-687">
<ul>
<li class="data-line-687">
<p>Utilizando os programas <a href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/canny.cpp" data-href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/canny.cpp">canny.cpp</a> e <a href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/pontilhismo.cpp" data-href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/pontilhismo.cpp">pontilhismo.cpp</a> como referÃªncia, implemente um programa cannypoints.cpp. A ideia Ã© usar as bordas produzidas pelo algoritmo de Canny para melhorar a qualidade da imagem pontilhista gerada. A forma como a informaÃ§Ã£o de borda serÃ¡ usada Ã© livre. Entretanto, sÃ£o apresentadas algumas sugestÃµes de tÃ©cnicas que poderiam ser utilizadas:</p>
<div class="ulist data-line-688">
<ul>
<li class="data-line-688">
<p>Desenhar pontos grandes na imagem pontilhista bÃ¡sica;</p>
</li>
<li class="data-line-689">
<p>Usar a posiÃ§Ã£o dos pixels de borda encontrados pelo algoritmo de Canny para desenhar pontos nos respectivos locais na imagem gerada.</p>
</li>
<li class="data-line-690">
<p>Experimente ir aumentando os limiares do algoritmo de Canny e, para cada novo par de limiares, desenhar cÃ­rculos cada vez menores nas posiÃ§Ãµes encontradas. A Figura 19 foi desenvolvida usando essa tÃ©cnica.</p>
</li>
</ul>
</div>
</li>
<li class="data-line-691">
<p>Escolha uma imagem de seu gosto e aplique a tÃ©cnica que vocÃª desenvolveu.</p>
</li>
<li class="data-line-692">
<p>Descreva no seu relatÃ³rio detalhes do procedimento usado para criar sua tÃ©cnica pontilhista.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-694">
<p>Neste programa, iremos simular a tÃ©cnica do pontilhismo desenhando digitalmente pequenos cÃ­rculos na imagem escolhida. Estes cÃ­rculos serÃ£o separados por pequenos intervalos e deslocados de seu centro de forma randÃ´mica.</p>
</div>
<div class="paragraph data-line-696">
<p>Inicialmente, definiremos a funÃ§Ã£o <code>pontilhismo()</code> para aplicaÃ§Ã£o da tÃ©cnica desejada. As variÃ¡veis <code>STEP</code>, <code>JITTER</code> e <code>RAIO</code> sÃ£o determinadas de antemÃ£o. Detalhando melhor esses parÃ¢metros, temos que <code>STEP</code> define o passo usado para varrer a imagem de referÃªncia, <code>JITTER</code> regula o intevalo de separaÃ§Ã£o (espalhamento) dos elementos e <code>RAIO</code> corresponde a distÃ¢ncia entre um ponto de cada cÃ­rculo gerado e seu centro.</p>
</div>
<div class="paragraph data-line-698">
<p><code>xrange</code> e <code>yrange</code> sÃ£o <em>arrays</em> de Ã­ndices que armazenam as coordenadas dos pontos em que vÃ£o ser colocados os cÃ­rculos do pontilhismo, preenchidos com valores sequenciais iniciando em 0, alÃ©m de receberem um ganho igual a <code>STEP</code> e um deslocamento <code>STEP//2</code>.</p>
</div>
<div class="listingblock data-line-701">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def pontilhismo(image):
    STEP = 4
    JITTER = 4
    RAIO = 3

    points = image.copy()
    rows, cols = image.shape[:-1]

    xrange = np.arange(0, rows-STEP, STEP) + STEP//2
    yrange = np.arange(0, cols-STEP, STEP) + STEP//2

    np.random.shuffle(xrange)

    for i in xrange:
        np.random.shuffle(yrange)
        for j in yrange:
            x = i + np.random.randint(2*JITTER) - JITTER + 1
            y = j + np.random.randint(2*JITTER) - JITTER + 1
            color = tuple(map(int, image[x, y]))
            points = cv2.circle(points, (y, x), RAIO, color, -1, lineType=cv2.LINE_AA)
    return</code></pre>
</div>
</div>
<div class="paragraph data-line-725">
<p>ApÃ³s embaralhar aleatoriamente os pontos atravÃ©s da funÃ§Ã£o <code>random.shuffle()</code>, os <em>loops</em> mostrados acima fazem com que as variÃ¡veis <code>i</code> e <code>j</code> assumam, a cada iteraÃ§Ã£o, os valores dos arrays <code>xrange</code> e <code>yrange</code> de forma consecutiva.</p>
</div>
<div class="paragraph data-line-727">
<p>A imagem que serÃ¡ utilizada para aplicaÃ§Ã£o da tÃ©cnica de pontilhismo Ã© mostrada abaixo:</p>
</div>
<div class="imageblock data-line-730">
<div class="content">
<img src="./imagens/jardim.png" alt="jardim">
</div>
<div class="title">Figure 24. Imagem original</div>
</div>
<div class="paragraph data-line-732">
<p>Para melhorar a qualidade das imagens que geraremos com a tÃ©cnica do pontilhismo, podemos utilizar a detecÃ§Ã£o de bordas de Canny. O algoritmo de Canny Ã© um dos mais rÃ¡pidos e eficientes algoritmos de detecÃ§Ã£o de bordas ou descontinuidades. A saÃ­da desse algoritmo Ã© uma imagem binÃ¡ria em que todas as bordas sÃ£o representadas com valor 255 (branco) e os demais pixels com valor 0 (preto).</p>
</div>
<div class="paragraph data-line-734">
<p>A <em>trackbar</em> estÃ¡ associada Ã  funÃ§Ã£o <code>on_trackbar_canny()</code>. Nessa funÃ§Ã£o, sÃ£o gerados cÃ­rculos a partir dos pixels de borda detectados, de modo a evidenciar os contornos. Com isso, denota-se que o limiar \(T_2\) empregado Ã© determinado pelo usuÃ¡rio a partir da interaÃ§Ã£o com o <em>slider</em>, enquanto o \(T_1\) Ã© obtido considerando a proporÃ§Ã£o de 3:1.</p>
</div>
<div class="listingblock data-line-737">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def on_trackbar_canny(slider):
    global img_final
    img_final = pontos.copy()

    for raio in [4,3,2,1]:
        fator = (5-raio)/2
        bordas = cv2.Canny(img, fator*slider, 3*fator*slider)

        px, py = np.where(bordas == 255)

        for i in range(0, len(px), 2):
            x, y = px[i], py[i]
            color = tuple(map(int, img[x, y]))
            img_final = cv2.circle(img_final, (y, x), raio, color, -1, lineType=cv2.LINE_AA)

    cv2.imshow('canny', img_final)</code></pre>
</div>
</div>
<div class="paragraph data-line-756">
<p>O processo Ã© repetido quatro vezes e, Ã  medida em que aumentam-se os limiares, sÃ£o criados cÃ­rculos menores. Para impedir a repetiÃ§Ã£o excessiva, os cÃ­rculos sÃ£o desenhados alternadamente sobre os pontos que compÃµem as bordas da imagem, evitando os pontos consecutivos.</p>
</div>
<div class="paragraph data-line-758">
<p>ApÃ³s aplicaÃ§Ã£o, os resultados com variaÃ§Ã£o de <em>threshold</em> podem ser observados abaixo:</p>
</div>
<div class="imageblock data-line-761">
<div class="content">
<img src="./imagens/resultados/jardim_pontilhismo.gif" alt="jardim pontilhismo">
</div>
<div class="title">Figure 25. Resultado com variaÃ§Ã£o de threshold</div>
</div>
<div class="paragraph data-line-763">
<p>A seguir, visualizamos outro teste realizado e a imagem aplicada a tÃ©cnica de pontilhismo, com <em>threshold</em> de 20.</p>
</div>
<div class="imageblock data-line-766">
<div class="content">
<img src="./imagens/resultados/outono_pontilhismo.png" alt="outono pontilhismo">
</div>
<div class="title">Figure 26. Entrada e saÃ­da do programa utilizando threshold de 20</div>
</div>
<div class="paragraph data-line-768">
<p>O cÃ³digo completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/cannypontilhism.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/cannypontilhism.py">cannypontilhism.py</a>.</p>
</div>
</div>
</div>
<div class="sect2 data-line-770">
<h3 id="_9_2_exercÃ­cios">9.2 ExercÃ­cios</h3>
<div class="sect3 data-line-772">
<h4 id="_9_2_1_clusterizaÃ§Ã£o_com_k_means">9.2.1 ClusterizaÃ§Ã£o com <em>k-means</em></h4>
<div class="ulist data-line-774">
<ul>
<li class="data-line-774">
<p>Utilizando o programa <a href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/kmeans.cpp" data-href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/kmeans.cpp">kmeans.cpp</a> como exemplo, prepare um programa exemplo onde a execuÃ§Ã£o do cÃ³digo se dÃª usando o parÃ¢metro nRodadas=1 e inciar os centros de forma aleatÃ³ria usando o parÃ¢metro KMEANS_RANDOM_CENTERS ao invÃ©s de KMEANS_PP_CENTERS. Realize 10 rodadas diferentes do algoritmo e compare as imagens produzidas. Explique porque elas podem diferir tanto.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-776">
<p>O <em>k-means</em> Ã© um processo de quantizaÃ§Ã£o que visa classificar <em>N</em> observaÃ§Ãµes em <em>K</em> aglomerados. Ã um processo iterativo em que extrai-se a distÃ¢ncia mÃ©dia das amostras em cada aglomerado, redefinindo as posiÃ§Ãµes dos centroides a cada passo. Esse processo acontece atÃ© nÃ£o termos mais mudanÃ§as significativas nas posiÃ§Ãµes dos centroides.</p>
</div>
<div class="paragraph data-line-778">
<p>No programa em questÃ£o, implementaremos o procedimento descrito acima comeÃ§ando com centroides aleatoriamente determinados, devido Ã  escolha do parÃ¢metro <code>KMEANS_RANDOM_CENTERS</code>. Definido na variÃ¡vel <code>criteria</code>, o critÃ©rio de parada se dÃ¡ ao atingir o nÃºmero mÃ¡ximo de 10000 iteraÃ§Ãµes ou o erro \(\epsilon\) de valor 0,0001.</p>
</div>
<div class="listingblock data-line-781">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_MAX_ITER, 10000, 0.0001)

for n in range(1, 11):
    _, rotulos, centros = cv2.kmeans(samples, nClusters, None, criteria, nRodadas, cv2.KMEANS_RANDOM_CENTERS)

    centros = np.uint8(centros)

    rotulada = np.zeros(img.shape, dtype=img.dtype)
    for y in range(rows):
        for x in range(cols):
            indice = rotulos[y + x*rows, 0]
            for z in range(3):
                rotulada[y, x][z] = centros[indice][z]</code></pre>
</div>
</div>
<div class="paragraph data-line-797">
<p>Conforme solicitado, o algoritmo Ã© rodado dez vezes, gerando dez imagens distintas. Como os centros dos agrupamentos sÃ£o escolhidos, inicialmente, de forma aleatÃ³ria, esses pontos mudarÃ£o a cada rodada, promovendo imagens relativamente diferentes. Isto ocorre devido Ã  natureza nÃ£o determinÃ­stica do mÃ©todo <em>k-means</em>, a qual pode levar a diferentes valores de convergÃªncia de acordo com os pontos de partida ou, em alguns casos, sequer encontrar uma convergÃªncia.</p>
</div>
<div class="paragraph data-line-799">
<p>As imagens geradas a partir da Figura 24 foram aglomeradas em um arquivo <em>.gif</em>, a fim de compararmos os resultados obtidos em cada iteraÃ§Ã£o.</p>
</div>
<div class="imageblock data-line-802">
<div class="content">
<img src="./imagens/resultados/clustered_img.gif" alt="clustered img">
</div>
<div class="title">Figure 27. Imagens clusterizadas</div>
</div>
<div class="paragraph data-line-804">
<p>O cÃ³digo completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/kmeans.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/kmeans.py">kmeans.py</a>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1 data-line-806">
<h2 id="_3Âª_unidade">3Âª unidade</h2>
<div class="sectionbody">
<div class="sect2 data-line-808">
<h3 id="_projeto_final_scanner_inteligente">Projeto final (<em>Scanner</em> inteligente)</h3>
<div class="sect3 data-line-810">
<h4 id="_proposiÃ§Ã£o">ProposiÃ§Ã£o</h4>
<div class="paragraph data-line-812">
<p>Sabe-se que os seres humanos possuem a capacidade de compreender o conteÃºdo de uma imagem apenas pela observaÃ§Ã£o e interpretaÃ§Ã£o de sÃ­mbolos. Assim, podemos facilmente reconhecer um texto contido em uma imagem e fazer a sua leitura.</p>
</div>
<div class="paragraph data-line-814">
<p>No dia a dia, frequentemente necessitamos da digitalizaÃ§Ã£o dessas informaÃ§Ãµes, principalmente quando lidamos com documentos, fazendo-se necessÃ¡rio processar, extrair o texto e armazenÃ¡-lo de forma editÃ¡vel. Nesse contexto, Ã© bastante conveniente e torna-se muito mais fÃ¡cil e rÃ¡pido fazer determinados trabalhos, pesquisar uma dada informaÃ§Ã£o ou manipular o conteÃºdo de um relatÃ³rio, por exemplo, quando fazemos essa conversÃ£o de forma automÃ¡tica.</p>
</div>
<div class="paragraph data-line-816">
<p>Assim, o algoritmo do <em>Scanner</em> Inteligente foi idealizado com o intuito de extrair automaticamente textos impressos e dados de documentos digitalizados em tempo real. Esse recurso integra os assuntos abordados na disciplina de Processamento Digital de Imagens e faz o reconhecimento Ã³ptico de caracteres (OCR), buscando o melhor enquadramento e disposiÃ§Ã£o do conteÃºdo do documento de interesse e convertendo o texto detectado nas imagens em arquivo de texto (<em>.txt</em>), usando bibliotecas Python.</p>
</div>
</div>
<div class="sect3 data-line-818">
<h4 id="_implementaÃ§Ã£o">ImplementaÃ§Ã£o</h4>
<div class="paragraph data-line-820">
<p>A descriÃ§Ã£o detalhada da implementaÃ§Ã£o deste algoritmo, bem como os resultados obtidos, podem ser acessados clicando <a href="https://brunaeloisa.github.io/pdi/scanner" data-href="https://brunaeloisa.github.io/pdi/scanner">aqui</a>.</p>
</div>
<div class="imageblock data-line-823">
<div class="content">
<img src="./projeto/example_scanner.png" alt="example scanner">
</div>
<div class="title">Figure 28. Scanner de documentos inteligente</div>
</div>
<div class="paragraph data-line-825">
<p>O cÃ³digo completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/scanner.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/scanner.py">scanner.py</a>.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2022-07-18 01:47:29 -0300
</div>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/highlight.min.js"></script>
<script>
if (!hljs.initHighlighting.called) {
  hljs.initHighlighting.called = true
  ;[].slice.call(document.querySelectorAll('pre.highlight > code')).forEach(function (el) { hljs.highlightBlock(el) })
}
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains("stemblock")) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>