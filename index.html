<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.17">
<meta name="author" content="André Varela, Bruna Soares">
<title>Processamento Digital de Imagens (DCA0445)</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/*! Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment the following line when using as a custom stylesheet */
/* @import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700"; */
html{font-family:sans-serif;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
b,strong{font-weight:bold}
abbr{font-size:.9em}
abbr[title]{cursor:help;border-bottom:1px dotted #dddddf;text-decoration:none}
dfn{font-style:italic}
hr{height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type=button],input[type=reset],input[type=submit]{-webkit-appearance:button;appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type=checkbox],input[type=radio]{padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,::before,::after{box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;line-height:1;position:relative;cursor:auto;-moz-tab-size:4;-o-tab-size:4;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:1px solid #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;border-radius:3px;box-shadow:0 1px 0 rgba(0,0,0,.2),inset 0 0 0 .1em #fff;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin:0 auto;max-width:62.5em;zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:flex;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border:1px solid #e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:hsla(0,0%,100%,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details{margin-left:1.25rem}
details>summary{cursor:pointer;display:block;position:relative;line-height:1.6;margin-bottom:.625rem;outline:none;-webkit-tap-highlight-color:transparent}
details>summary::-webkit-details-marker{display:none}
details>summary::before{content:"";border:solid transparent;border-left:solid;border-width:.3em 0 .3em .5em;position:absolute;top:.5em;left:-1.25rem;transform:translateX(15%)}
details[open]>summary::before{border:solid transparent;border-top:solid;border-width:.5em .3em 0;transform:translateY(15%)}
details>summary::after{content:"";width:1.25rem;height:1em;position:absolute;top:.3em;left:-1.25rem}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class=paragraph]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border:1px solid #e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border:1px solid #dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class=highlight],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos,pre.pygments .linenos{border-right:1px solid;opacity:.35;padding-right:.5em;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}
pre.pygments span.linenos{display:inline-block;margin-right:.75em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans-serif;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all>*>tr,table.stripes-odd>*>tr:nth-of-type(odd),table.stripes-even>*>tr:nth-of-type(even),table.stripes-hover>*>tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
li>p:empty:only-child::before{content:"";display:inline-block}
ul.checklist>li>p:first-child{margin-left:-1em}
ul.checklist>li>p:first-child>.fa-square-o:first-child,ul.checklist>li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist>li>p:first-child>input[type=checkbox]:first-child{margin-right:.25em}
ul.inline{display:flex;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:4px solid #fff;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt,summary{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt,summary{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]{border-bottom:1px dotted}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media amzn-kf8,print{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/styles/github.min.css">
</head>
<body class="article toc2 toc-left data-line-1">
<div id="header">
<h1>Processamento Digital de Imagens (DCA0445)</h1>
<div class="details">
<span id="author" class="author">André Varela</span><br>
<span id="email" class="email"><a href="mailto:andre.varela.104@ufrn.edu.br" data-href="mailto:andre.varela.104@ufrn.edu.br">andre.varela.104@ufrn.edu.br</a></span><br>
<span id="author2" class="author">Bruna Soares</span><br>
<span id="email2" class="email"><a href="mailto:brunaeloisa7@gmail.com" data-href="mailto:brunaeloisa7@gmail.com">brunaeloisa7@gmail.com</a></span><br>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Sumário</div>
<ul class="sectlevel1">
<li><a href="#_sobre_o_opencv">Sobre o OpenCV</a></li>
<li><a href="#_1ª_unidade">1ª Unidade</a>
<ul class="sectlevel2">
<li><a href="#_2_2_exercícios">2.2. Exercícios</a>
<ul class="sectlevel3">
<li><a href="#_2_2_1_região_negativa">2.2.1 Região negativa</a></li>
<li><a href="#_2_2_2_troca_de_quadrantes">2.2.2 Troca de quadrantes</a></li>
</ul>
</li>
<li><a href="#_3_2_exercícios">3.2. Exercícios</a>
<ul class="sectlevel3">
<li><a href="#_3_2_1_rotulação_de_objetos">3.2.1 Rotulação de objetos</a></li>
<li><a href="#_3_2_2_contagem_de_objetos">3.2.2 Contagem de objetos</a></li>
</ul>
</li>
<li><a href="#_4_2_exercícios">4.2. Exercícios</a>
<ul class="sectlevel3">
<li><a href="#_4_2_1_equalização_de_histograma">4.2.1 Equalização de histograma</a></li>
<li><a href="#_4_2_2_detector_de_movimento">4.2.2 Detector de movimento</a></li>
</ul>
</li>
<li><a href="#_5_2_exercícios">5.2. Exercícios</a>
<ul class="sectlevel3">
<li><a href="#_5_2_1_laplaciano_do_gaussiano">5.2.1 Laplaciano do gaussiano</a></li>
</ul>
</li>
<li><a href="#_6_1_exercícios">6.1. Exercícios</a>
<ul class="sectlevel3">
<li><a href="#_6_1_1_tilt_shift_em_imagem">6.1.1 <em>Tilt-shift</em> em imagem</a></li>
<li><a href="#_6_1_2_tilt_shift_em_vídeo">6.1.2 <em>Tilt-shift</em> em vídeo</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_2ª_unidade">2ª Unidade</a>
<ul class="sectlevel2">
<li><a href="#_7_2_exercícios">7.2. Exercícios</a>
<ul class="sectlevel3">
<li><a href="#_7_2_1_filtro_homomórfico">7.2.1 Filtro homomórfico</a></li>
</ul>
</li>
<li><a href="#_8_3_exercícios">8.3 Exercícios</a>
<ul class="sectlevel3">
<li><a href="#_8_3_1_pontilhismo">8.3.1 Pontilhismo</a></li>
</ul>
</li>
<li><a href="#_9_2_exercícios">9.2 Exercícios</a>
<ul class="sectlevel3">
<li><a href="#_9_2_1_clusterização_com_k_means">9.2.1 Clusterização com <em>k-means</em></a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#_3ª_unidade">3ª unidade</a>
<ul class="sectlevel2">
<li><a href="#_projeto_final_scanner_inteligente">Projeto final (<em>Scanner</em> inteligente)</a>
<ul class="sectlevel3">
<li><a href="#_proposição">Proposição</a></li>
<li><a href="#_implementação">Implementação</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div class="sect1 data-line-10">
<h2 id="_sobre_o_opencv">Sobre o OpenCV</h2>
<div class="sectionbody">
<div class="paragraph data-line-12">
<p>Para a execução dos códigos citados nesta página foi utilizado o OpenCV (<em>Open Source Computer Vision</em>), que é uma biblioteca <em>open-source</em> muito usada em aplicações de visão computacional, processamento de imagens e vídeos. Na linguagem Python, esta biblioteca possui uma boa integração com o módulo numérico NumPy, o qual oferece suporte para criação de matrizes multi-dimensionais e um amplo leque de funções para manipulação de <em>arrays</em>. Nas versões mais recentes do OpenCV, esses objetos <code>ndarray</code> já são utilizados nativamente como uma forma de otimização.</p>
</div>
</div>
</div>
<div class="sect1 data-line-14">
<h2 id="_1ª_unidade">1ª Unidade</h2>
<div class="sectionbody">
<div class="sect2 data-line-16">
<h3 id="_2_2_exercícios">2.2. Exercícios</h3>
<div class="sect3 data-line-18">
<h4 id="_2_2_1_região_negativa">2.2.1 Região negativa</h4>
<div class="ulist data-line-19">
<ul>
<li class="data-line-19">
<p>Implementar um programa capaz de solicitar ao usuário as coordenadas de dois pontos P1 e P2 localizados dentro dos limites do tamanho da imagem e exibir que lhe for fornecida. Entretanto, a região definida pelo retângulo de vértices opostos definidos pelos pontos P1 e P2 será exibida com o negativo da imagem na região correspondente.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-21">
<p>A imagem utilizada como base está sendo mostrada na Figura 1.</p>
</div>
<div class="imageblock data-line-24">
<div class="content">
<img src="./imagens/biel.png" alt="biel">
</div>
<div class="title">Figure 1. biel.png</div>
</div>
<div class="paragraph data-line-26">
<p>Apesar da imagem em questão ser representada em tons de cinza, a solução deste exercício foi pensada para uma imagem genérica e, portanto, optamos por ler a imagem em matrizes de cores utilizando a flag <code>cv2.IMREAD_COLOR</code>.</p>
</div>
<div class="listingblock data-line-29">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">img_orig = cv2.imread('biel.png', cv2.IMREAD_COLOR)</code></pre>
</div>
</div>
<div class="paragraph data-line-33">
<p>Extraímos, então, as dimensões da imagem e o número de canais utilizando o atributo <code>shape</code>.</p>
</div>
<div class="listingblock data-line-36">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">altura, largura, canais = img_orig.shape</code></pre>
</div>
</div>
<div class="paragraph data-line-40">
<p>Na sequência, solicitamos ao usuário as coordenadas dos pontos de interesse P1 e P2. A partir das coordenadas fornecidas, percorremos os pixels que compõem o retângulo entre esses dois pontos, invertendo os tons de todos os canais.</p>
</div>
<div class="listingblock data-line-43">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">for i in range(min(p1x, p2x), max(p1x, p2x)+1):
    for j in range(min(p1y, p2y), max(p1y, p2y)+1):
        for k in range(canais):
            img_neg[i,j,k] = 255 - img_neg[i,j,k]</code></pre>
</div>
</div>
<div class="paragraph data-line-50">
<p>Para exibir o resultado, precisou-se utilizar a função <code>cv2_imshow()</code>, a qual é uma versão compatível com o Google Colab de <code>cv2.imshow()</code>. O resultado final tomando-se a Figura 1 como entrada e considerando os pontos P1=(90, 190) e P2=(220, 120) está exposto na Figura 2.</p>
</div>
<div class="imageblock data-line-53">
<div class="content">
<img src="./imagens/resultados/biel_neg.png" alt="biel neg">
</div>
<div class="title">Figure 2. Região negativa em imagem sugerida</div>
</div>
<div class="paragraph data-line-55">
<p>Aplicando como entrada uma imagem colorida, obtivemos o seguinte resultado.</p>
</div>
<div class="imageblock data-line-58">
<div class="content">
<img src="./imagens/resultados/beatles_neg.png" alt="beatles neg">
</div>
<div class="title">Figure 3. Região negativa em imagem colorida</div>
</div>
</div>
<div class="sect3 data-line-62">
<h4 id="_2_2_2_troca_de_quadrantes">2.2.2 Troca de quadrantes</h4>
<div class="ulist data-line-63">
<ul>
<li class="data-line-63">
<p>Implementar um programa que troque os quadrantes em diagonal na imagem.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-65">
<p>Da mesma forma que o exercício anterior, lemos a imagem e extraímos os seus dados essenciais. Vale ressaltar que para que a troca dos quadrantes seja possível as dimensões da imagem devem ser pares. Adicionamos, portanto, duas condicionais: caso uma das duas dimensões da imagem seja ímpar, descartaremos a última linha e/ou coluna do <em>array</em>.</p>
</div>
<div class="listingblock data-line-68">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">if altura % 2 != 0:
    altura -= 1
    img_orig = img_orig[:-1,::]
if largura % 2 != 0:
    largura -= 1
    img_orig = img_orig[::,:-1]</code></pre>
</div>
</div>
<div class="paragraph data-line-77">
<p>Em seguida, calculamos a metade das dimensões para uso posterior e criamos um novo <em>array</em> para armazenar a imagem trocada.</p>
</div>
<div class="paragraph data-line-79">
<p>Por fim, invertemos os quadrantes da figura selecionando as regiões da imagem de origem e atribuindo-as às suas novas posições utilizando <em>slicing</em> de <em>arrays</em>, como mostra o trecho de código abaixo.</p>
</div>
<div class="listingblock data-line-82">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># quarto quadrante = segundo quadrante
img_trocada[meia_altura:,meia_largura:] = img_orig[:meia_altura,:meia_largura]
# terceiro quadrante = primeiro quadrante
img_trocada[meia_altura:,:meia_largura] = img_orig[:meia_altura,meia_largura:]
# primeiro quadrante = terceiro quadrante
img_trocada[:meia_altura,meia_largura:] = img_orig[meia_altura:,:meia_largura]
# segundo quadrante = quarto quadrante
img_trocada[:meia_altura,:meia_largura] = img_orig[meia_altura:,meia_largura:]</code></pre>
</div>
</div>
<div class="paragraph data-line-93">
<p>Exibimos, então, o resultado armazenado em <code>img_trocada</code>, obtendo a figura final, como pode ser visto a seguir.</p>
</div>
<div class="imageblock data-line-96">
<div class="content">
<img src="./imagens/resultados/biel_trocada.png" alt="biel trocada">
</div>
<div class="title">Figure 4. Troca de quadrantes em imagem sugerida</div>
</div>
<div class="paragraph data-line-98">
<p>Como a lógica do programa foi pensada para uma imagem genérica, aplicou-se uma figura colorida com dimensões de 814 x 543 pixels, a fim de verificar o seu funcionamento. O resultado é apresentado na Figura 5.</p>
</div>
<div class="imageblock data-line-101">
<div class="content">
<img src="./imagens/resultados/lago_trocada.png" alt="lago trocada">
</div>
<div class="title">Figure 5. Troca de quadrantes em imagem colorida</div>
</div>
<div class="paragraph data-line-103">
<p>O código completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/trocaregioes.ipynb" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/trocaregioes.ipynb">trocaregioes.ipynb</a>.</p>
</div>
</div>
</div>
<div class="sect2 data-line-105">
<h3 id="_3_2_exercícios">3.2. Exercícios</h3>
<div class="sect3 data-line-107">
<h4 id="_3_2_1_rotulação_de_objetos">3.2.1 Rotulação de objetos</h4>
<div class="ulist data-line-108">
<ul>
<li class="data-line-108">
<p>Observando-se o programa <a href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/labeling.cpp" data-href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/labeling.cpp">labeling.cpp</a> como exemplo, é possível verificar que caso existam mais de 255 objetos na cena, o processo de rotulação poderá ficar comprometido. Identifique a situação em que isso ocorre e proponha uma solução para este problema.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-110">
<p>Existe uma limitação de valores em 255 tons de cinza distintos em razão do tipo de variável que está sendo utilizada (<code>unsigned char</code>). A proposta de solução é utilizar uma matriz do tipo <code>int</code> ou <code>float</code> para aumentar as possibilidades de contabilização. Para além disso, poderíamos também adaptar o algoritmo para o sistema RGB, possibilitando a criação de 255³ rótulos, que são diferenciáveis na visão computacional.</p>
</div>
</div>
<div class="sect3 data-line-112">
<h4 id="_3_2_2_contagem_de_objetos">3.2.2 Contagem de objetos</h4>
<div class="ulist data-line-113">
<ul>
<li class="data-line-113">
<p>Aprimore o algoritmo de contagem apresentado para identificar regiões com ou sem buracos internos que existam na cena. Assuma que objetos com mais de um buraco podem existir. Inclua suporte no seu algoritmo para não contar bolhas que tocam as bordas da imagem. Não se pode presumir, a priori, que elas tenham buracos ou não.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-115">
<p>Como devemos levar em consideração que podem existir bolhas com mais de um buraco, a imagem a ser utilizada como base para este código é mostrada na Figura 6.</p>
</div>
<div class="imageblock data-line-118">
<div class="content">
<img src="./imagens/bolhas.png" alt="bolhas">
</div>
<div class="title">Figure 6. Entrada do programa</div>
</div>
<div class="paragraph data-line-120">
<p>Inicialmente, lê-se o arquivo e as dimensões da imagem. Logo após, percorremos todos os pixels de borda, aplicando um <em>flood fill</em> da cor do fundo (tom 0) sempre que nos depararmos com um pixel branco no percurso.</p>
</div>
<div class="listingblock data-line-123">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">for i in range(altura):
    if img_aux[i,0] == 255:
        cv2.floodFill(img_aux, None, (0,i), 0)
        num_bolhas_borda += 1
    if img_aux[i,largura-1] == 255:
        cv2.floodFill(img_aux, None, (largura-1,i), 0)
        num_bolhas_borda += 1
for j in range(largura):
    if img_aux[0,j] == 255:
        cv2.floodFill(img_aux, None, (j,0), 0)
        num_bolhas_borda += 1
    if img_aux[altura-1,j] == 255:
        cv2.floodFill(img_aux, None, (j,altura-1), 0)
        num_bolhas_borda += 1</code></pre>
</div>
</div>
<div class="paragraph data-line-140">
<p>A variável <code>num_bolhas_borda</code> armazena o número de bolhas que foram excluídas da figura e é incrementada sempre que chamamos a função <code>cv2.floodFill()</code>. A nova imagem, dada por <code>img_aux</code>, é mostrada na Figura 7.</p>
</div>
<div class="imageblock data-line-143">
<div class="content">
<img src="./imagens/resultados/bolhas1.png" alt="bolhas1">
</div>
<div class="title">Figure 7. Imagem sem bolhas de borda</div>
</div>
<div class="paragraph data-line-145">
<p>Na sequência, faremos o <em>labeling</em> das bolhas presentes na nova imagem. Para isso, percorremos todos os pixels da figura, procurando novamente por pixels brancos. Ao encontrarmos uma nova bolha, incrementamos a variável <code>num_bolhas</code> e aplicamos o <em>flood fill</em> com o tom correspondente ao valor atual do contador.</p>
</div>
<div class="listingblock data-line-148">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">num_bolhas = 0
for i in range(altura):
    for j in range(largura):
        if img_aux[i,j] == 255:
            num_bolhas += 1
            cv2.floodFill(img_aux, None, (j,i), num_bolhas)</code></pre>
</div>
</div>
<div class="paragraph data-line-157">
<p>Ao fim deste processo, conheceremos a quantidade total de bolhas e cada uma delas estará sendo representada por um tom de cinza diferente, como exposto na Figura 8.</p>
</div>
<div class="imageblock data-line-160">
<div class="content">
<img src="./imagens/resultados/bolhas2.png" alt="bolhas2">
</div>
<div class="title">Figure 8. Labeling das bolhas</div>
</div>
<div class="paragraph data-line-162">
<p>Para realizar a contagem das bolhas com buracos, iniciamos pintando o fundo de branco, aplicando um <em>flood fill</em> no ponto (0,0), de modo que apenas os buracos apresentem tom de cinza 0.</p>
</div>
<div class="imageblock data-line-165">
<div class="content">
<img src="./imagens/resultados/bolhas3.png" alt="bolhas3">
</div>
<div class="title">Figure 9. Alteração da cor de fundo</div>
</div>
<div class="paragraph data-line-167">
<p>Varremos novamente a figura, dessa vez buscando os pixels pretos e aplicando neles um <em>flood fill</em> com a cor do fundo. Verificamos, então, o pixel antecedente: caso este seja diferente de 220, incrementamos a variável <code>num_bolhas_buracos</code> e pintamos a bolha que contém o buraco em questão com o tom de cinza 220; caso contrário, temos que a bolha já foi contabilizada.</p>
</div>
<div class="listingblock data-line-170">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">num_bolhas_buracos = 0
for i in range(1, altura):
    for j in range(1, largura):
        if img_aux[i,j] == 0:
            cv2.floodFill(img_aux, None, (j,i), 255)
            if img_aux[i,j-1] != 220:
                cv2.floodFill(img_aux, None, (j-1,i), 220)
                num_bolhas_buracos += 1</code></pre>
</div>
</div>
<div class="paragraph data-line-181">
<p>O trecho de código acima garantirá que nenhuma bolha seja contabilizada mais de uma vez e fará com que as bolhas com buracos fiquem destacadas na figura, como vemos abaixo.</p>
</div>
<div class="imageblock data-line-184">
<div class="content">
<img src="./imagens/resultados/bolhas4.png" alt="bolhas4">
</div>
<div class="title">Figure 10. Indentificação das bolhas com buracos</div>
</div>
<div class="paragraph data-line-186">
<p>Obtemos, enfim, a seguinte saída: <code>A figura tem 7 bolhas com buracos e 14 bolhas sem buracos</code>. O código completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/labeling.ipynb" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/labeling.ipynb">labeling.ipynb</a>.</p>
</div>
</div>
</div>
<div class="sect2 data-line-188">
<h3 id="_4_2_exercícios">4.2. Exercícios</h3>
<div class="sect3 data-line-190">
<h4 id="_4_2_1_equalização_de_histograma">4.2.1 Equalização de histograma</h4>
<div class="ulist data-line-191">
<ul>
<li class="data-line-191">
<p>Implementar um programa que deverá, para cada imagem capturada, realizar a equalização do histograma antes de exibir a imagem. Teste sua implementação apontando a câmera para ambientes com iluminações variadas e observando o efeito gerado. Assuma que as imagens processadas serão em tons de cinza.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-193">
<p>Para realizar a captura de imagens através de uma câmera (<em>webcam</em>), utilizamos a função <code>cv2.VideoCapture()</code>.</p>
</div>
<div class="listingblock data-line-196">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)</code></pre>
</div>
</div>
<div class="paragraph data-line-200">
<p>Em segunda instância, definimos o tamanho da tela de exibição como <em>(512 x 480)</em> e o número de tonalidades como 256, podendo apresentar valores de 0 a 255 (o segundo valor de <code>hist_range</code> é <em>exclusive</em>).</p>
</div>
<div class="listingblock data-line-203">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">hist_size = 256
hist_w, hist_h = 512, 480
hist_range = (0, 256)</code></pre>
</div>
</div>
<div class="paragraph data-line-209">
<p>O parâmetro <code>bin_w</code> define a largura ocupada por cada tom de cinza no histograma.</p>
</div>
<div class="listingblock data-line-212">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">bin_w = int(round(hist_w/hist_size))</code></pre>
</div>
</div>
<div class="paragraph data-line-216">
<p>Na sequência, convertemos cada frame capturado para escala de cinza e realizamos a equalização com a função <code>cv.equalizeHist()</code>. A partir de agora, os processos ocorrerão em <em>loop</em> para todos os frames coletados em tempo real enquanto não existir uma condição de interrupção do programa.</p>
</div>
<div class="listingblock data-line-219">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
frame_eq = cv2.equalizeHist(frame)</code></pre>
</div>
</div>
<div class="paragraph data-line-224">
<p>Após a equalização dos tons de cinza, faremos o cálculo do histograma por meio da função <code>cv2.calcHist()</code> empregando os parâmetros previamente definidos como argumentos. Depois disso, normalizaremos os histogramas gerados com a função <code>cv2.normalize()</code>.</p>
</div>
<div class="listingblock data-line-227">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">hist = cv2.calcHist([frame], [0], None, [hist_size], hist_range)
cv2.normalize(hist, hist, alpha=0, beta=hist_h, norm_type=cv2.NORM_MINMAX)

hist_eq = cv2.calcHist([frame_eq], [0], None, [hist_size], hist_range)
cv2.normalize(hist_eq, hist_eq, alpha=0, beta=hist_h, norm_type=cv2.NORM_MINMAX)</code></pre>
</div>
</div>
<div class="paragraph data-line-235">
<p>Com isso, criamos um conjunto de barras verticais (representação de um trem de pulsos), que partem do eixo horizontal e vão até o ponto do valor calculado, a fim de representar a quantidade de pixels de cada tom de cinza do frame.</p>
</div>
<div class="listingblock data-line-238">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">for i in range(hist_size):
    cv2.line(histImage, (bin_w*i, hist_h - int(hist[i])), (bin_w*i, hist_h), 255, thickness=2)
    cv2.line(histImage_eq, (bin_w*i, hist_h - int(hist_eq[i])), (bin_w*i, hist_h), 255, thickness=2)</code></pre>
</div>
</div>
<div class="paragraph data-line-244">
<p>Exibimos, então, a captura gerada pela <em>webcam</em> e o seu histograma:</p>
</div>
<div class="imageblock data-line-247">
<div class="content">
<img src="./imagens/resultados/normal.png" alt="normal">
</div>
<div class="title">Figure 11. Captura original da webcam</div>
</div>
<div class="paragraph data-line-249">
<p>Após a aplicação da equalização com o nosso programa, geramos o seguinte resultado:</p>
</div>
<div class="imageblock data-line-252">
<div class="content">
<img src="./imagens/resultados/equalizado.png" alt="equalizado">
</div>
<div class="title">Figure 12. Captura equalizada</div>
</div>
<div class="paragraph data-line-254">
<p>Nota-se que, como a captura foi realizada em um ambiente com boa iluminação, o histograma se encontra concentrado em uma região mais clara. Após equalização, percebemos uma distribuiçao mais uniforme das intensidades dos pixels na imagem gerada e um melhor constraste.</p>
</div>
<div class="paragraph data-line-256">
<p>Para apreciar melhor os resultados da equalização, vamos introduzir uma imagem com pouco contraste, sendo a captura realizada em local com baixa iluminação. Note que os pixels estão agrupados, predominantemente, no lado mais a esquerda do histograma.</p>
</div>
<div class="imageblock data-line-259">
<div class="content">
<img src="./imagens/resultados/normal_escuro.png" alt="normal escuro">
</div>
<div class="title">Figure 13. Captura original da webcam em ambiente de baixa iluminação</div>
</div>
<div class="paragraph data-line-261">
<p>Abaixo visualizamos o resultado da equalização e do novo histograma obtido. Verifica-se uma clara redistribuição dos pixels imagem por todo o histograma.</p>
</div>
<div class="paragraph data-line-263">
<p>Apesar de conseguirmos visualizar melhor alguns detalhes da imagem equalizada em comparação com a imagem escura, o ruído está muito presente.</p>
</div>
<div class="imageblock data-line-266">
<div class="content">
<img src="./imagens/resultados/equalizado_escuro.png" alt="equalizado escuro">
</div>
<div class="title">Figure 14. Captura equalizada em ambiente de baixa iluminação</div>
</div>
<div class="paragraph data-line-268">
<p>O código completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/equalize.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/equalize.py">equalize.py</a>.</p>
</div>
</div>
<div class="sect3 data-line-270">
<h4 id="_4_2_2_detector_de_movimento">4.2.2 Detector de movimento</h4>
<div class="ulist data-line-271">
<ul>
<li class="data-line-271">
<p>Implementar um programa que deverá continuamente calcular o histograma da imagem (apenas uma componente de cor é suficiente) e compará-lo com o último histograma calculado. Ative um alarme quando a diferença entre estes ultrapassar um limiar pré-estabelecido, utilizando a função de comparação que julgar conveniente.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-273">
<p>Este programa utiliza as alterações no histograma dos frames do vídeo capturado para detecção de movimentos. Partindo de uma estrutura semelhante ao do algoritmo anterior, calcularemos, para cada frame, o histograma da componente vermelha e o compararemos com o histograma que o antecedeu.</p>
</div>
<div class="paragraph data-line-275">
<p>Precisamos, inicialmente, separar os canais da imagem:</p>
</div>
<div class="listingblock data-line-278">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">bgr = cv2.split(frame)</code></pre>
</div>
</div>
<div class="paragraph data-line-282">
<p>Assim, calculamos e normalizamos o histograma atual do canal vermelho.</p>
</div>
<div class="listingblock data-line-285">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">hist_atual = cv2.calcHist(bgr, [2], None, [hist_size], hist_range)
cv2.normalize(hist_atual, hist_atual, alpha=0, beta=hist_h, norm_type=cv2.NORM_MINMAX)</code></pre>
</div>
</div>
<div class="paragraph data-line-290">
<p>Para a condição inicial em que não temos um frame anterior para comparação, inicializamos uma variável <code>inicio = True</code> e atribuímos a <code>hist_anterior</code> o histograma atual que está sendo gerado. Terminada esta ação, a variável <code>inicio</code> passa a ser <code>False</code>.</p>
</div>
<div class="listingblock data-line-293">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">if inicio:
    hist_anterior = hist_atual.copy()
    inicio = False</code></pre>
</div>
</div>
<div class="paragraph data-line-299">
<p>Os histogramas são comparados por meio da função <code>compareHist()</code> e o resultado é inserido na varivável <code>dif</code>, que representa o quanto o histograma do frame atual difere do anterior. Para tal, utilizamos a métrica <code>cv2.HISTCMP_BHATTACHARYYA</code>, a qual retorna valores entre 0.0 e 1.0, em que quanto mais próximo de 0.0, maior é a semelhança entre as imagens.</p>
</div>
<div class="paragraph data-line-301">
<p>A partir disso, comparamos o valor encontrado (em %) ao limiar, que foi definido como 3 após alguns testes para alcançar a sensibilidade desejada. Quando este é ultrapassado, uma mensagem de <code>Movimento detectado !</code> é exibida na imagem.</p>
</div>
<div class="listingblock data-line-304">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">dif = cv2.compareHist(hist_anterior, hist_atual, cv2.HISTCMP_BHATTACHARYYA)
    if (100 * dif &gt; 3):
        cv2.putText(frame, "Movimento detectado!", (10, 30), cv2.FONT_HERSHEY_TRIPLEX, 1, (0, 0, 255))</code></pre>
</div>
</div>
<div class="paragraph data-line-310">
<p>O resultado pode ser observado abaixo:</p>
</div>
<div class="imageblock data-line-313">
<div class="content">
<img src="./imagens/resultados/motiondetector.gif" alt="motiondetector">
</div>
<div class="title">Figure 15. Detecção de movimento</div>
</div>
<div class="paragraph data-line-315">
<p>O código completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/motiondetector.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/motiondetector.py">motiondetector.py</a>.</p>
</div>
</div>
</div>
<div class="sect2 data-line-317">
<h3 id="_5_2_exercícios">5.2. Exercícios</h3>
<div class="sect3 data-line-319">
<h4 id="_5_2_1_laplaciano_do_gaussiano">5.2.1 Laplaciano do gaussiano</h4>
<div class="ulist data-line-320">
<ul>
<li class="data-line-320">
<p>Utilizando o programa <a href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/filtroespacial.cpp" data-href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/filtroespacial.cpp">filtroespacial.cpp</a> como referência, implementar um programa que acrescente mais uma funcionalidade ao exemplo fornecido, permitindo que seja calculado o laplaciano do gaussiano das imagens capturadas. Compare o resultado desse filtro com a simples aplicação do filtro laplaciano.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-322">
<p>Inicialmente, adaptamos o código citado no exercício para a linguagem Python. Nele, o usuário pode escolher o efeito (módulo, média, gaussiano, bordas horizontais, bordas verticais, laplaciano e <em>boost</em>) que deseja aplicar às imagens capturadas em tempo real por meio das teclas do teclado.</p>
</div>
<div class="listingblock data-line-325">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">key = cv2.waitKey(10)

if key == 27:
    break
elif key == ord('a'):
    absolut = not absolut
  elif key == ord('m'):
    laplacian_gauss = False
    mask = np.float32(media)
  elif key == ord('g'):
    laplacian_gauss = False
    mask = np.float32(gauss)
  elif key == ord('h'):
    laplacian_gauss = False
    mask = np.float32(horizontal)
  elif key == ord('v'):
    laplacian_gauss = False
    mask = np.float32(vertical)
  elif key == ord('l'):
    laplacian_gauss = False
    mask = np.float32(laplacian)
  elif key == ord('b'):
    laplacian_gauss = False
    mask = np.float32(boost)
  elif key == ord('f'):
    laplacian_gauss = True
    mask = np.float32(gauss)
  elif key == ord('s'):
    cv2.imwrite('filtered_img.png', filtro_espacial)
    print("Imagem salva em filtered_img.png")</code></pre>
</div>
</div>
<div class="paragraph data-line-358">
<p>Nota-se que foram adicionadas duas novas funcionalidades ao programa: o filtro laplaciano do gaussiano e o salvamento em arquivo do frame atual com filtro aplicado.</p>
</div>
<div class="paragraph data-line-360">
<p>Para realizar o laplaciano do gaussiano, foi criada uma <em>flag</em> (<code>laplacian_gauss</code>) que retorna <code>True</code> quando o comando (tecla <em>f</em>) é ativado. Dessa forma, primeiramente selecionamos a máscara adequada para aplicar o filtro gaussiano e, em seguida, criamos uma expressão condicional baseada na <em>flag</em> em questão a fim de aplicar o laplaciano em cima da imagem já filtrada previamente.</p>
</div>
<div class="listingblock data-line-363">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
frame_gray = cv2.flip(frame_gray, 1)

frame_32f = np.float32(frame_gray)
filtered_frame = cv2.filter2D(frame_32f, -1, mask)

if laplacian_gauss:
    filtered_frame = cv2.filter2D(filtered_frame, -1, laplacian)</code></pre>
</div>
</div>
<div class="paragraph data-line-374">
<p>Quando outras opções de filtro são escolhidas, a variável <code>laplacian_gauss</code> é setada para <code>False</code> e, portanto, nenhum processo adicional é realizado no frame.</p>
</div>
<div class="paragraph data-line-376">
<p>Na sequência, verificamos se os valores mostrados devem ser absolutos ou não e transformamos os dados da matriz resultante que representa a imagem final em valores <code>unsigned</code> de 8 bits (0 a 255), para, só então, exibí-la em uma janela, assim como a imagem original.</p>
</div>
<div class="listingblock data-line-379">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">if absolut:
    filtered_frame = np.abs(filtered_frame)

filtro_espacial = np.uint8(filtered_frame)

cv2.imshow('imagem original', frame_gray)
cv2.imshow('imagem com efeito', filtro_espacial)</code></pre>
</div>
</div>
<div class="paragraph data-line-389">
<p>Para comparar os efeitos do filtro laplaciano e laplaciano do gaussiano, utilizamos como entrada do programa a imagem exposta a seguir.</p>
</div>
<div class="imageblock data-line-392">
<div class="content">
<img src="./imagens/livro.png" alt="livro">
</div>
<div class="title">Figure 16. Imagem utilizada no programa</div>
</div>
<div class="paragraph data-line-394">
<p>O comparativo entre os dois resultados é mostrado na Figura 17. Percebe-se que, ao aplicarmos o filtro gaussiano, que realiza um borramento, antes do laplaciano, atenuamos os ruídos da imagem tornando as bordas mais precisas na figura resultante, dando a impressão de uma imagem mais escura.</p>
</div>
<div class="imageblock data-line-397">
<div class="content">
<img src="./imagens/resultados/laplgauss_livro.png" alt="laplgauss livro">
</div>
<div class="title">Figure 17. Comparativo entre os filtros laplaciano e laplaciano do gaussiano</div>
</div>
<div class="paragraph data-line-399">
<p>O código completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/laplgauss.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/laplgauss.py">laplgauss.py</a>.</p>
</div>
</div>
</div>
<div class="sect2 data-line-401">
<h3 id="_6_1_exercícios">6.1. Exercícios</h3>
<div class="sect3 data-line-403">
<h4 id="_6_1_1_tilt_shift_em_imagem">6.1.1 <em>Tilt-shift</em> em imagem</h4>
<div class="ulist data-line-404">
<ul>
<li class="data-line-404">
<p>Utilizando o programa <a href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/addweighted.cpp" data-href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/addweighted.cpp">addweighted.cpp</a> como referência, implementar um programa no qual três ajustes deverão ser providos na tela da interface:</p>
<div class="ulist data-line-405">
<ul>
<li class="data-line-405">
<p>um ajuste para regular a altura da região central que entrará em foco;</p>
</li>
<li class="data-line-406">
<p>um ajuste para regular a força de decaimento da região borrada;</p>
</li>
<li class="data-line-407">
<p>um ajuste para regular a posição vertical do centro da região que entrará em foco. Finalizado o programa, a imagem produzida deverá ser salva em arquivo.</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="paragraph data-line-409">
<p>Para este código, inicialmente lemos a imagem de entrada do programa, armazenando-a na variável <code>img1</code>. Em seguida, a <code>img2</code> é obtida aplicando-se cinco vezes o filtro da média com máscara 5x5 na imagem original, a fim de alcançar um efeito de borramento com intensidade desejável.</p>
</div>
<div class="listingblock data-line-412">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">img_32f = np.float32(img2)
for n in range(5):
    img_32f = cv2.filter2D(img_32f, -1, mask)
img2 = np.uint8(img_32f)</code></pre>
</div>
</div>
<div class="paragraph data-line-419">
<p>Criamos a janela e também as barras de ajuste solicitadas no exercício utilizando a função <code>cv2.createTrackbar()</code>, definindo um <em>slider</em> que inicia em 0 e pode ir até 100. Um dos parâmetros exigidos para esta criação é uma função de <em>callback</em>, a qual será chamada sempre que o usuário interagir com a barra de rolagem.</p>
</div>
<div class="listingblock data-line-422">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">cv2.namedWindow('Tiltshift')
cv2.createTrackbar('altura', 'Tiltshift', slider_inicial, slider_max, on_trackbar)
cv2.createTrackbar('centro', 'Tiltshift', slider_inicial, slider_max, on_trackbar)
cv2.createTrackbar('decaimento', 'Tiltshift', slider_inicial, slider_max, on_trackbar)</code></pre>
</div>
</div>
<div class="paragraph data-line-429">
<p>Ao chamar o <em>callback</em>, a função apresentada acima automaticamente envia como argumento o valor atual da barra que foi ajustada. Por esse motivo, a função <code>on_trackbar()</code> possui um argumento (<code>val</code>), mas este não é utilizado em seu interior, uma vez que optamos por usar a mesma função de <em>callback</em> para todas as três barras. Como alternativa, coletamos a posição atual de todos os <em>sliders</em> localmente, utilizando a função <code>cv2.getTrackbarPos()</code> sempre que há alteração.</p>
</div>
<div class="listingblock data-line-432">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def on_trackbar(val):
    global img_final

    slider_altura = cv2.getTrackbarPos('altura', 'Tiltshift')  # altura da região central
    slider_centro = cv2.getTrackbarPos('centro', 'Tiltshift')  # posição vertical do centro
    slider_decaimento = cv2.getTrackbarPos('decaimento', 'Tiltshift')

    l1 = slider_centro - int(slider_altura/2)
    l2 = slider_centro + int(slider_altura/2)

    if l1 &gt;= 0 and l2 &lt;= 100:
        l1 = l1*height/100
        l2 = l2*height/100
    else:
        return

    aux = img1.copy()

    x = np.arange(height, dtype=np.float32)

    alpha = 0.5 * (np.tanh((x - l1)/(slider_decaimento+0.001)) - np.tanh((x - l2)/(slider_decaimento+0.001)))

    for i, element in enumerate(alpha):
        aux[i] = cv2.addWeighted(img1[i], element, img2[i], 1 - element, 0.0)

    cv2.imshow('Tiltshift', aux)
    img_final = aux</code></pre>
</div>
</div>
<div class="paragraph data-line-462">
<p>Para simular o efeito do <em>tilt-shift</em>, fazemos uma soma ponderada entre a imagem e sua versão borrada por meio da função <code>cv2.addWeighted()</code>. Esse processo pode ser modelado por uma função que define a região de desfoque ao longo do eixo vertical da imagem:</p>
</div>
<div class="stemblock data-line-465">
<div class="content">
\[\alpha(x) = \frac{1}{2}\left(\tanh{\frac{x-l_1}{d}}-\tanh{\frac{x-l_2}{d}} \right)\]
</div>
</div>
<div class="paragraph data-line-469">
<p>em que \(x\) representa as linhas da imagem, \(l_1\) e \(l_2\) são os limites verticais para a região sem borramento cujo \(\alpha\) assume valor em torno de 0.5 e \(d\) indica a força do decaimento da região normal para a região borrada.</p>
</div>
<div class="paragraph data-line-471">
<p>Para evitar o erro de divisão por zero, foi acrescido ao decaimento um valor de 0.001. Além disso, antes de atribuir os novos parâmetros, verificamos se os valores de altura da região sem borramento e o centro determinado são compatíveis para que o tamanho da imagem não seja excedido.</p>
</div>
<div class="paragraph data-line-473">
<p>Ao final, exibimos na janela o resultado dessa combinação linear e o armazenamos na variável global <code>img_final</code>, a fim de disponibilizar a imagem atual para ser salva em arquivo pelo comando <em>s</em>.</p>
</div>
<div class="paragraph data-line-475">
<p>Para verificar o funcionamento do código, utilizamos a imagem abaixo como entrada do programa.</p>
</div>
<div class="imageblock data-line-478">
<div class="content">
<img src="./imagens/cidade.jpg" alt="cidade">
</div>
<div class="title">Figure 18. Imagem de entrada para o <em>tilt-shift</em></div>
</div>
<div class="paragraph data-line-480">
<p>Ao realizarmos o ajuste das barras (altura: 70; centro: 48; decaimento: 22), chegamos no resultado apresentado na Figura 19.</p>
</div>
<div class="imageblock data-line-483">
<div class="content">
<img src="./imagens/resultados/img_tiltshift.png" alt="img tiltshift">
</div>
<div class="title">Figure 19. Imagem de saída para o <em>tilt-shift</em></div>
</div>
<div class="paragraph data-line-485">
<p>O código completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/tiltshift.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/tiltshift.py">tiltshift.py</a>.</p>
</div>
</div>
<div class="sect3 data-line-487">
<h4 id="_6_1_2_tilt_shift_em_vídeo">6.1.2 <em>Tilt-shift</em> em vídeo</h4>
<div class="ulist data-line-488">
<ul>
<li class="data-line-488">
<p>Implementar um programa capaz de processar um arquivo de vídeo, produzir o efeito de <em>tilt-shift</em> nos quadros presentes e escrever o resultado em outro arquivo de vídeo. A ideia é criar um efeito de miniaturização de cenas. Descarte quadros em uma taxa que julgar conveniente para evidenciar o efeito de <em>stop motion</em>, comum em vídeos desse tipo.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-490">
<p>Este caso é uma aplicação do exemplo acima, em que cada quadro do vídeo é processado como realizado anteriormente e alguns quadros são descartados para criar um efeito de <em>stop motion</em>.</p>
</div>
<div class="paragraph data-line-492">
<p>O vídeo utilizado para exemplificação está disposto abaixo.</p>
</div>
<div class="videoblock data-line-495">
<div class="title">Vídeo original</div>
<div class="content">
<video src="./videos/video.mp4" controls>
Your browser does not support the video tag.
</video>
</div>
</div>
<div class="paragraph data-line-497">
<p>O algoritmo usará os parâmetros de ajuste que são fornecidos pelo usuário através da mudança das barras de rolagem para calcular os valores de \(\alpha\), que é um vetor do tamanho da imagem. Esse efeito, que a princípio é gerado no frame inicial, repercutirá por todos os frames restantes compondo o vídeo final.</p>
</div>
<div class="listingblock data-line-500">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">cap = cv2.VideoCapture(arq_video)

# lê primeiro frame para ajustar parâmetros do tilt-shift
ret, frame1 = cap.read()</code></pre>
</div>
</div>
<div class="paragraph data-line-507">
<p>Como o mesmo efeito será aplicado para todos os frames do vídeo, uma vez definida a configuração, não será necessário recalcular os parâmetros. Por esse motivo, tratamos o vetor \(\alpha\) como uma variável global que será constantemente atualizada pela função de <em>callback</em> das barras para, enfim, ser utilizada dentro da função <code>tiltshift()</code> repetidas vezes durante a criação do vídeo.</p>
</div>
<div class="paragraph data-line-509">
<p>Essa função recebe como argumento o frame original e o frame borrado e retorna a imagem com o efeito do <em>tilt-shift</em> aplicado. O seu algoritmo segue a mesma lógica que o exercício anterior.</p>
</div>
<div class="listingblock data-line-512">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def tiltshift(frame1, frame2):
    img = frame1.copy()

    for i, element in enumerate(alpha):
        img[i] = cv2.addWeighted(
            frame1[i], element, frame2[i], 1 - element, 0.0)

    return img</code></pre>
</div>
</div>
<div class="paragraph data-line-523">
<p>Como requisito do programa, a função <code>salvarvideo()</code> grava o resultado do efeito aplicado no vídeo. O objetivo da função <code>cv2.VideoWriter()</code> é, portanto, salvar o vídeo final em arquivo, definindo o nome (<code>video_tiltshift.mp4</code>), extensão, taxa de frames por segundo e as dimensões do vídeo (iguais ao original).</p>
</div>
<div class="paragraph data-line-525">
<p>Para criar o efeito de <em>stop motion</em>, uma condição <code>descarta</code> é imposta e a variável <code>taxa</code> determinará esse descarte. Nesse processo, os frames que são múltiplos do valor armazenado em <code>taxa</code> devem ser mantidos, ou seja, a cada 8 frames do vídeo original, extraímos 1 frame para o vídeo final.</p>
</div>
<div class="listingblock data-line-528">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def salvarvideo(video):
    taxa = 8
    descarta = 0

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter('video_tiltshift.mp4', fourcc, 15, (width, height))

    cap = cv2.VideoCapture(video)

    while True:
        ret, frame1 = cap.read()
        if ret:
            if descarta == 0:
                frame2 = frame1.copy()

                # aplica borramento
                frame_32f = np.float32(frame2)
                for _ in range(5):
                    frame_32f = cv2.filter2D(frame_32f, -1, mask)
                frame2 = np.uint8(frame_32f)

                novo_frame = tiltshift(frame1, frame2)
                out.write(novo_frame)

                descarta += 1
                descarta = descarta % taxa
            else:
                descarta += 1
                descarta = descarta % taxa
        else:
            break

    out.release()
    print("Vídeo salvo como video_tiltshift.mp4")</code></pre>
</div>
</div>
<div class="paragraph data-line-565">
<p>Quando o usuário concluir a sua configuração, ele pode pressionar a tecla <em>s</em> para salvar o resultado no arquivo de vídeo <code>video_tiltshift.mp4</code>.</p>
</div>
<div class="listingblock data-line-568">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">k = cv2.waitKey(0)
if k == 27:
    cv2.destroyAllWindows()
elif k == ord('s'):
    salvarvideo(arq_video)
    cv2.destroyAllWindows()</code></pre>
</div>
</div>
<div class="paragraph data-line-577">
<p>O resultado do efeito <em>tilt-shift</em> no vídeo mostrado previamente pode ser visualizado abaixo.</p>
</div>
<div class="imageblock data-line-580">
<div class="content">
<img src="./imagens/resultados/video_tiltshift.gif" alt="video tiltshift">
</div>
<div class="title">Figure 20. Vídeo resultante com <em>tilt-shift</em></div>
</div>
<div class="paragraph data-line-582">
<p>O código completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/tiltshiftvideo.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/tiltshiftvideo.py">tiltshiftvideo.py</a>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1 data-line-584">
<h2 id="_2ª_unidade">2ª Unidade</h2>
<div class="sectionbody">
<div class="sect2 data-line-586">
<h3 id="_7_2_exercícios">7.2. Exercícios</h3>
<div class="sect3 data-line-588">
<h4 id="_7_2_1_filtro_homomórfico">7.2.1 Filtro homomórfico</h4>
<div class="ulist data-line-589">
<ul>
<li class="data-line-589">
<p>Utilizando o programa <a href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/dft.cpp" data-href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/dft.cpp">dft.cpp</a> como referência, implementar o filtro homomórfico para melhorar imagens com iluminação irregular. Assumindo que a imagem fornecida é em tons de cinza, crie uma cena mal iluminada e ajuste os parâmetros do filtro homomórfico para corrigir a iluminação da melhor forma possível.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-591">
<p>Neste exercício, resolveremos problemas de distribuição irregular de iluminação em imagens utilizando o filtro homomórfico.</p>
</div>
<div class="paragraph data-line-593">
<p>O filtro homomórfico é uma versão modificada do filtro gaussiano e atua sobre as componentes de reflectância e iluminância da imagem, a partir da seguinte equação matemática:</p>
</div>
<div class="stemblock data-line-596">
<div class="content">
\[H(u,v)=(\gamma _{H}-\gamma _{L})(1-e^{-c(D^{2}(u,v)/D_{0}^{2})})+\gamma _{L}\]
</div>
</div>
<div class="paragraph data-line-600">
<p>Os parâmetros do filtro (\(\gamma_{H}\), \(\gamma_{L}\), \(D_{0}\) e \(c\)) são ajustados pelo usuário a fim atingir o resultado desejado: reduzir os efeitos causados pela má iluminação na imagem original. O algoritmo implementado segue os passos descritos a seguir.</p>
</div>
<div class="paragraph data-line-602">
<p>Primeiramente, realiza-se o <em>padding</em> da imagem e aplica-se a transformada de Fourier, transferindo a análise para o domínio da frequência.</p>
</div>
<div class="paragraph data-line-604">
<p>Em seguida, desloca-se o centro do seu espectro, invertendo os quadrantes da imagem (A↔D, B↔C) por meio da função <code>deslocaDFT()</code>.</p>
</div>
<div class="listingblock data-line-607">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def deslocaDFT(image):
    # se a imagem tiver tamanho ímpar, recorta a regiao para evitar cópias de tamanho desigual
    image = image[0:(image.shape[0] &amp; -2), 0:(image.shape[1] &amp; -2)]

    cy = int(image.shape[0] / 2)
    cx = int(image.shape[1] / 2)

    # reorganiza os quadrantes da transformada
    # A B   -&gt;  D C
    # C D       B A
    A = image[:cy, :cx].copy()
    B = image[:cy, cx:].copy()
    C = image[cy:, :cx].copy()
    D = image[cy:, cx:].copy()

    # A &lt;-&gt; D
    image[:cy, :cx] = D
    image[cy:, cx:] = A

    # C &lt;-&gt; B
    image[:cy, cx:] = C
    image[cy:, :cx] = B

    return image</code></pre>
</div>
</div>
<div class="paragraph data-line-634">
<p>Na sequência, multiplica-se a função na frequência pelo filtro (\(G(u,v) = H(u,v)F(u,v)\)) e retoma-se a configuração original dos quadrantes da imagem invertendo nocamente os quadrantes.</p>
</div>
<div class="paragraph data-line-636">
<p>Finalmente, calcula-se a transformada inversa do produto e extrai-se a parte real da imagem no domínio espacial.</p>
</div>
<div class="paragraph data-line-638">
<p>Detendo-se ao filtro em si, definimos a função <code>filtro_homomorfico()</code> que implementará o equacionamento matemático. Como comentado anteriormente, o algoritmo usará os parâmetros do filtro, os quais são fornecidos pelo usuário através do controle das barras deslizantes, que variam de 0 a 100.</p>
</div>
<div class="listingblock data-line-641">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def filtro_homomorfico():
    gammaH = cv2.getTrackbarPos('gama H', 'filtro homomorfico') / 100
    gammaL = cv2.getTrackbarPos('gama L', 'filtro homomorfico') / 100
    D0 = cv2.getTrackbarPos('D0', 'filtro homomorfico')
    c = cv2.getTrackbarPos('c', 'filtro homomorfico') / 10</code></pre>
</div>
</div>
<div class="paragraph data-line-649">
<p>A variável <code>tmp</code> corresponde à matriz temporária utilizada para criar as componentes real e imaginária do filtro. Com isso, define-se <em>D(u,v)</em> e implemeta-se o filtro, exibindo o resultado em janela. Enfim, cria-se a matriz com as componentes do filtro e combina ambas em uma matriz multicanal complexa.</p>
</div>
<div class="listingblock data-line-652">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">    tmp = np.zeros((dft_M, dft_N), dtype=np.float32)
    v = np.array(range(dft_N))

    for u in range(dft_M):
        D = np.sqrt((u-dft_M/2)**2 + (v-dft_N/2)**2)
        tmp[u] = (gammaH-gammaL)*(1-np.exp(-c*(D**2)/(0.0001+D0**2))) + gammaL

    cv2.imshow('filtro', tmp)

    comps = [tmp, tmp]
    filtro_h = cv2.merge(comps)

    return filtro_h</code></pre>
</div>
</div>
<div class="paragraph data-line-668">
<p>Para exemplificar, foi gerado um filtro com os seguintes parâmetros: \(\gamma_{H}=89\), \(\gamma_{L}=35\), \(D_{0}=70\) e \(c=96\), obtendo o resultando abaixo. Verifica-se que este facilita a passagem de componentes de frequências mais altas (reflectância) em detrimento das frequências mais baixas (iluminância).</p>
</div>
<div class="imageblock data-line-671">
<div class="content">
<img src="./imagens/filtro_h.png" alt="filtro h">
</div>
<div class="title">Figure 21. Filtro homomórfico gerado pelos parâmetros de entrada</div>
</div>
<div class="paragraph data-line-673">
<p>A imagem original utilizada de teste é apresentada na Figura 22. Em seguida, é mostrada a imagem após a realização da filtragem homomórfica.</p>
</div>
<div class="imageblock data-line-676">
<div class="content">
<img src="./imagens/entrada_homomorfico.png" alt="entrada homomorfico">
</div>
<div class="title">Figure 22. Imagem original com problema de iluminação</div>
</div>
<div class="imageblock data-line-679">
<div class="content">
<img src="./imagens/resultados/foto_corrigida.png" alt="foto corrigida">
</div>
<div class="title">Figure 23. Imagem com parâmetros do filtro homomórfico aplicados</div>
</div>
<div class="paragraph data-line-681">
<p>O código completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/homomorfico.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/homomorfico.py">homomorfico.py</a>.</p>
</div>
</div>
</div>
<div class="sect2 data-line-683">
<h3 id="_8_3_exercícios">8.3 Exercícios</h3>
<div class="sect3 data-line-685">
<h4 id="_8_3_1_pontilhismo">8.3.1 Pontilhismo</h4>
<div class="ulist data-line-687">
<ul>
<li class="data-line-687">
<p>Utilizando os programas <a href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/canny.cpp" data-href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/canny.cpp">canny.cpp</a> e <a href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/pontilhismo.cpp" data-href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/pontilhismo.cpp">pontilhismo.cpp</a> como referência, implemente um programa cannypoints.cpp. A ideia é usar as bordas produzidas pelo algoritmo de Canny para melhorar a qualidade da imagem pontilhista gerada. A forma como a informação de borda será usada é livre. Entretanto, são apresentadas algumas sugestões de técnicas que poderiam ser utilizadas:</p>
<div class="ulist data-line-688">
<ul>
<li class="data-line-688">
<p>Desenhar pontos grandes na imagem pontilhista básica;</p>
</li>
<li class="data-line-689">
<p>Usar a posição dos pixels de borda encontrados pelo algoritmo de Canny para desenhar pontos nos respectivos locais na imagem gerada.</p>
</li>
<li class="data-line-690">
<p>Experimente ir aumentando os limiares do algoritmo de Canny e, para cada novo par de limiares, desenhar círculos cada vez menores nas posições encontradas. A Figura 19 foi desenvolvida usando essa técnica.</p>
</li>
</ul>
</div>
</li>
<li class="data-line-691">
<p>Escolha uma imagem de seu gosto e aplique a técnica que você desenvolveu.</p>
</li>
<li class="data-line-692">
<p>Descreva no seu relatório detalhes do procedimento usado para criar sua técnica pontilhista.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-694">
<p>Neste programa, iremos simular a técnica do pontilhismo desenhando digitalmente pequenos círculos na imagem escolhida. Estes círculos serão separados por pequenos intervalos e deslocados de seu centro de forma randômica.</p>
</div>
<div class="paragraph data-line-696">
<p>Inicialmente, definiremos a função <code>pontilhismo()</code> para aplicação da técnica desejada. As variáveis <code>STEP</code>, <code>JITTER</code> e <code>RAIO</code> são determinadas de antemão. Detalhando melhor esses parâmetros, temos que <code>STEP</code> define o passo usado para varrer a imagem de referência, <code>JITTER</code> regula o intevalo de separação (espalhamento) dos elementos e <code>RAIO</code> corresponde a distância entre um ponto de cada círculo gerado e seu centro.</p>
</div>
<div class="paragraph data-line-698">
<p><code>xrange</code> e <code>yrange</code> são <em>arrays</em> de índices que armazenam as coordenadas dos pontos em que vão ser colocados os círculos do pontilhismo, preenchidos com valores sequenciais iniciando em 0, além de receberem um ganho igual a <code>STEP</code> e um deslocamento <code>STEP//2</code>.</p>
</div>
<div class="listingblock data-line-701">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def pontilhismo(image):
    STEP = 4
    JITTER = 4
    RAIO = 3

    points = image.copy()
    rows, cols = image.shape[:-1]

    xrange = np.arange(0, rows-STEP, STEP) + STEP//2
    yrange = np.arange(0, cols-STEP, STEP) + STEP//2

    np.random.shuffle(xrange)

    for i in xrange:
        np.random.shuffle(yrange)
        for j in yrange:
            x = i + np.random.randint(2*JITTER) - JITTER + 1
            y = j + np.random.randint(2*JITTER) - JITTER + 1
            color = tuple(map(int, image[x, y]))
            points = cv2.circle(points, (y, x), RAIO, color, -1, lineType=cv2.LINE_AA)
    return</code></pre>
</div>
</div>
<div class="paragraph data-line-725">
<p>Após embaralhar aleatoriamente os pontos através da função <code>random.shuffle()</code>, os <em>loops</em> mostrados acima fazem com que as variáveis <code>i</code> e <code>j</code> assumam, a cada iteração, os valores dos arrays <code>xrange</code> e <code>yrange</code> de forma consecutiva.</p>
</div>
<div class="paragraph data-line-727">
<p>A imagem que será utilizada para aplicação da técnica de pontilhismo é mostrada abaixo:</p>
</div>
<div class="imageblock data-line-730">
<div class="content">
<img src="./imagens/jardim.png" alt="jardim">
</div>
<div class="title">Figure 24. Imagem original</div>
</div>
<div class="paragraph data-line-732">
<p>Para melhorar a qualidade das imagens que geraremos com a técnica do pontilhismo, podemos utilizar a detecção de bordas de Canny. O algoritmo de Canny é um dos mais rápidos e eficientes algoritmos de detecção de bordas ou descontinuidades. A saída desse algoritmo é uma imagem binária em que todas as bordas são representadas com valor 255 (branco) e os demais pixels com valor 0 (preto).</p>
</div>
<div class="paragraph data-line-734">
<p>A <em>trackbar</em> está associada à função <code>on_trackbar_canny()</code>. Nessa função, são gerados círculos a partir dos pixels de borda detectados, de modo a evidenciar os contornos. Com isso, denota-se que o limiar \(T_2\) empregado é determinado pelo usuário a partir da interação com o <em>slider</em>, enquanto o \(T_1\) é obtido considerando a proporção de 3:1.</p>
</div>
<div class="listingblock data-line-737">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def on_trackbar_canny(slider):
    global img_final
    img_final = pontos.copy()

    for raio in [4,3,2,1]:
        fator = (5-raio)/2
        bordas = cv2.Canny(img, fator*slider, 3*fator*slider)

        px, py = np.where(bordas == 255)

        for i in range(0, len(px), 2):
            x, y = px[i], py[i]
            color = tuple(map(int, img[x, y]))
            img_final = cv2.circle(img_final, (y, x), raio, color, -1, lineType=cv2.LINE_AA)

    cv2.imshow('canny', img_final)</code></pre>
</div>
</div>
<div class="paragraph data-line-756">
<p>O processo é repetido quatro vezes e, à medida em que aumentam-se os limiares, são criados círculos menores. Para impedir a repetição excessiva, os círculos são desenhados alternadamente sobre os pontos que compõem as bordas da imagem, evitando os pontos consecutivos.</p>
</div>
<div class="paragraph data-line-758">
<p>Após aplicação, os resultados com variação de <em>threshold</em> podem ser observados abaixo:</p>
</div>
<div class="imageblock data-line-761">
<div class="content">
<img src="./imagens/resultados/jardim_pontilhismo.gif" alt="jardim pontilhismo">
</div>
<div class="title">Figure 25. Resultado com variação de threshold</div>
</div>
<div class="paragraph data-line-763">
<p>A seguir, visualizamos outro teste realizado e a imagem aplicada a técnica de pontilhismo, com <em>threshold</em> de 20.</p>
</div>
<div class="imageblock data-line-766">
<div class="content">
<img src="./imagens/resultados/outono_pontilhismo.png" alt="outono pontilhismo">
</div>
<div class="title">Figure 26. Entrada e saída do programa utilizando threshold de 20</div>
</div>
<div class="paragraph data-line-768">
<p>O código completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/cannypontilhism.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/cannypontilhism.py">cannypontilhism.py</a>.</p>
</div>
</div>
</div>
<div class="sect2 data-line-770">
<h3 id="_9_2_exercícios">9.2 Exercícios</h3>
<div class="sect3 data-line-772">
<h4 id="_9_2_1_clusterização_com_k_means">9.2.1 Clusterização com <em>k-means</em></h4>
<div class="ulist data-line-774">
<ul>
<li class="data-line-774">
<p>Utilizando o programa <a href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/kmeans.cpp" data-href="https://agostinhobritojr.github.io/tutorial/pdi/exemplos/kmeans.cpp">kmeans.cpp</a> como exemplo, prepare um programa exemplo onde a execução do código se dê usando o parâmetro nRodadas=1 e inciar os centros de forma aleatória usando o parâmetro KMEANS_RANDOM_CENTERS ao invés de KMEANS_PP_CENTERS. Realize 10 rodadas diferentes do algoritmo e compare as imagens produzidas. Explique porque elas podem diferir tanto.</p>
</li>
</ul>
</div>
<div class="paragraph data-line-776">
<p>O <em>k-means</em> é um processo de quantização que visa classificar <em>N</em> observações em <em>K</em> aglomerados. É um processo iterativo em que extrai-se a distância média das amostras em cada aglomerado, redefinindo as posições dos centroides a cada passo. Esse processo acontece até não termos mais mudanças significativas nas posições dos centroides.</p>
</div>
<div class="paragraph data-line-778">
<p>No programa em questão, implementaremos o procedimento descrito acima começando com centroides aleatoriamente determinados, devido à escolha do parâmetro <code>KMEANS_RANDOM_CENTERS</code>. Definido na variável <code>criteria</code>, o critério de parada se dá ao atingir o número máximo de 10000 iterações ou o erro \(\epsilon\) de valor 0,0001.</p>
</div>
<div class="listingblock data-line-781">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_MAX_ITER, 10000, 0.0001)

for n in range(1, 11):
    _, rotulos, centros = cv2.kmeans(samples, nClusters, None, criteria, nRodadas, cv2.KMEANS_RANDOM_CENTERS)

    centros = np.uint8(centros)

    rotulada = np.zeros(img.shape, dtype=img.dtype)
    for y in range(rows):
        for x in range(cols):
            indice = rotulos[y + x*rows, 0]
            for z in range(3):
                rotulada[y, x][z] = centros[indice][z]</code></pre>
</div>
</div>
<div class="paragraph data-line-797">
<p>Conforme solicitado, o algoritmo é rodado dez vezes, gerando dez imagens distintas. Como os centros dos agrupamentos são escolhidos, inicialmente, de forma aleatória, esses pontos mudarão a cada rodada, promovendo imagens relativamente diferentes. Isto ocorre devido à natureza não determinística do método <em>k-means</em>, a qual pode levar a diferentes valores de convergência de acordo com os pontos de partida ou, em alguns casos, sequer encontrar uma convergência.</p>
</div>
<div class="paragraph data-line-799">
<p>As imagens geradas a partir da Figura 24 foram aglomeradas em um arquivo <em>.gif</em>, a fim de compararmos os resultados obtidos em cada iteração.</p>
</div>
<div class="imageblock data-line-802">
<div class="content">
<img src="./imagens/resultados/clustered_img.gif" alt="clustered img">
</div>
<div class="title">Figure 27. Imagens clusterizadas</div>
</div>
<div class="paragraph data-line-804">
<p>O código completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/kmeans.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/kmeans.py">kmeans.py</a>.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1 data-line-806">
<h2 id="_3ª_unidade">3ª unidade</h2>
<div class="sectionbody">
<div class="sect2 data-line-808">
<h3 id="_projeto_final_scanner_inteligente">Projeto final (<em>Scanner</em> inteligente)</h3>
<div class="sect3 data-line-810">
<h4 id="_proposição">Proposição</h4>
<div class="paragraph data-line-812">
<p>Sabe-se que os seres humanos possuem a capacidade de compreender o conteúdo de uma imagem apenas pela observação e interpretação de símbolos. Assim, podemos facilmente reconhecer um texto contido em uma imagem e fazer a sua leitura.</p>
</div>
<div class="paragraph data-line-814">
<p>No dia a dia, frequentemente necessitamos da digitalização dessas informações, principalmente quando lidamos com documentos, fazendo-se necessário processar, extrair o texto e armazená-lo de forma editável. Nesse contexto, é bastante conveniente e torna-se muito mais fácil e rápido fazer determinados trabalhos, pesquisar uma dada informação ou manipular o conteúdo de um relatório, por exemplo, quando fazemos essa conversão de forma automática.</p>
</div>
<div class="paragraph data-line-816">
<p>Assim, o algoritmo do <em>Scanner</em> Inteligente foi idealizado com o intuito de extrair automaticamente textos impressos e dados de documentos digitalizados em tempo real. Esse recurso integra os assuntos abordados na disciplina de Processamento Digital de Imagens e faz o reconhecimento óptico de caracteres (OCR), buscando o melhor enquadramento e disposição do conteúdo do documento de interesse e convertendo o texto detectado nas imagens em arquivo de texto (<em>.txt</em>), usando bibliotecas Python.</p>
</div>
</div>
<div class="sect3 data-line-818">
<h4 id="_implementação">Implementação</h4>
<div class="paragraph data-line-820">
<p>A descrição detalhada da implementação deste algoritmo, bem como os resultados obtidos, podem ser acessados clicando <a href="https://brunaeloisa.github.io/pdi/scanner" data-href="https://brunaeloisa.github.io/pdi/scanner">aqui</a>.</p>
</div>
<div class="imageblock data-line-823">
<div class="content">
<img src="./projeto/example_scanner.png" alt="example scanner">
</div>
<div class="title">Figure 28. Scanner de documentos inteligente</div>
</div>
<div class="paragraph data-line-825">
<p>O código completo pode ser encontrado em: <a href="https://github.com/brunaeloisa/pdi/blob/main/codigos/scanner.py" data-href="https://github.com/brunaeloisa/pdi/blob/main/codigos/scanner.py">scanner.py</a>.</p>
</div>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Last updated 2022-07-18 01:47:29 -0300
</div>
</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.3/highlight.min.js"></script>
<script>
if (!hljs.initHighlighting.called) {
  hljs.initHighlighting.called = true
  ;[].slice.call(document.querySelectorAll('pre.highlight > code')).forEach(function (el) { hljs.highlightBlock(el) })
}
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains("stemblock")) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>